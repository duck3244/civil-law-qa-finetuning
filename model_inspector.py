# model_inspector.py
# ì €ì¥ëœ ëª¨ë¸ êµ¬ì¡° í™•ì¸ ë° ë””ë²„ê¹… ë„êµ¬

import os
import json
import torch
import safetensors
from pathlib import Path

def inspect_saved_model(model_path):
    """ì €ì¥ëœ ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ë‚´ìš© ë¶„ì„"""
    print(f"ğŸ” ëª¨ë¸ ê²€ì‚¬: {model_path}")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # íŒŒì¼ ëª©ë¡ í™•ì¸
    print("ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
    for file in os.listdir(model_path):
        file_path = os.path.join(model_path, file)
        size = os.path.getsize(file_path) / 1024 / 1024  # MB
        print(f"   {file} ({size:.2f} MB)")
    
    print("\n" + "=" * 60)
    
    # adapter_config.json í™•ì¸
    config_path = os.path.join(model_path, "adapter_config.json")
    if os.path.exists(config_path):
        print("âš™ï¸ LoRA ì„¤ì •:")
        with open(config_path, 'r') as f:
            config = json.load(f)
            for key, value in config.items():
                print(f"   {key}: {value}")
    else:
        print("âŒ adapter_config.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    print("\n" + "=" * 60)
    
    # adapter_model.safetensors í™•ì¸
    safetensors_path = os.path.join(model_path, "adapter_model.safetensors")
    if os.path.exists(safetensors_path):
        print("ğŸ”‘ LoRA ì–´ëŒ‘í„° í‚¤ë“¤:")
        try:
            from safetensors import safe_open
            with safe_open(safetensors_path, framework="pt", device="cpu") as f:
                keys = f.keys()
                print(f"   ì´ {len(keys)}ê°œ í‚¤ ë°œê²¬:")
                for i, key in enumerate(sorted(keys)):
                    if i < 10:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                        print(f"   - {key}")
                    elif i == 10:
                        print(f"   ... ì´ {len(keys)}ê°œ (ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ)")
                        break
                
                # ë¬¸ì œê°€ ë˜ëŠ” í‚¤ í™•ì¸
                embed_keys = [k for k in keys if "embed" in k]
                if embed_keys:
                    print(f"\nğŸ¯ Embedding ê´€ë ¨ í‚¤ë“¤:")
                    for key in embed_keys:
                        print(f"   - {key}")
                
        except Exception as e:
            print(f"âŒ safetensors íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        print("âŒ adapter_model.safetensors íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    print("\n" + "=" * 60)
    
    # tokenizer ê´€ë ¨ íŒŒì¼ í™•ì¸
    tokenizer_files = ["tokenizer.json", "tokenizer_config.json", "special_tokens_map.json"]
    print("ğŸ“ í† í¬ë‚˜ì´ì € íŒŒì¼ë“¤:")
    for file in tokenizer_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"   âœ… {file}")
        else:
            print(f"   âŒ {file}")

def check_base_model_compatibility(base_model_name, model_path):
    """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ LoRA ì–´ëŒ‘í„° í˜¸í™˜ì„± í™•ì¸"""
    print(f"\nğŸ”— í˜¸í™˜ì„± ê²€ì‚¬: {base_model_name}")
    print("=" * 60)
    
    try:
        from transformers import AutoModelForCausalLM, AutoConfig
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ì • ë¡œë“œ
        print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ì • ë¡œë“œ ì¤‘...")
        base_config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)
        
        print(f"ğŸ¤– ëª¨ë¸ íƒ€ì…: {base_config.model_type}")
        print(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {getattr(base_config, 'vocab_size', 'Unknown')}")
        print(f"ğŸ”¢ ë ˆì´ì–´ ìˆ˜: {getattr(base_config, 'num_hidden_layers', 'Unknown')}")
        
        # LoRA ì„¤ì •ê³¼ ë¹„êµ
        config_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                lora_config = json.load(f)
            
            print(f"\nâš™ï¸ LoRA ì„¤ì •:")
            print(f"   íƒ€ê²Ÿ ëª¨ë“ˆ: {lora_config.get('target_modules', 'Unknown')}")
            print(f"   ì €ì¥ ëª¨ë“ˆ: {lora_config.get('modules_to_save', 'Unknown')}")
            print(f"   ë² ì´ìŠ¤ ëª¨ë¸: {lora_config.get('base_model_name_or_path', 'Unknown')}")
            
            # í˜¸í™˜ì„± í™•ì¸
            saved_base_model = lora_config.get('base_model_name_or_path', '')
            if base_model_name in saved_base_model or saved_base_model in base_model_name:
                print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ í˜¸í™˜ì„±: ì¼ì¹˜")
            else:
                print(f"âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ë¶ˆì¼ì¹˜:")
                print(f"   í˜„ì¬ ì‚¬ìš©: {base_model_name}")
                print(f"   ì €ì¥ëœ ê²ƒ: {saved_base_model}")
        
    except Exception as e:
        print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")

def suggest_fixes(model_path):
    """ë¬¸ì œ í•´ê²° ë°©ì•ˆ ì œì‹œ"""
    print(f"\nğŸ’¡ í•´ê²° ë°©ì•ˆ")
    print("=" * 60)
    
    # adapter_config.jsonì—ì„œ ì •ë³´ ì¶”ì¶œ
    config_path = os.path.join(model_path, "adapter_config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        original_model = config.get('base_model_name_or_path', '')
        
        print(f"1. ì •í™•í•œ ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©:")
        print(f"   python main.py --mode test --model_path {model_path} --model_name {original_model}")
        
        print(f"\n2. í˜¸í™˜ ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ:")
        print(f"   python fix_model_loading.py --model_path {model_path}")
        
        print(f"\n3. ëª¨ë¸ ì¬ë³€í™˜:")
        print(f"   python convert_model.py --input {model_path} --output {model_path}_fixed")
        
    else:
        print("âŒ adapter_config.jsonì´ ì—†ì–´ ì •í™•í•œ ì§„ë‹¨ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        print("\nê¶Œì¥ì‚¬í•­:")
        print("1. ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨")
        print("2. ì €ì¥ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="ëª¨ë¸ êµ¬ì¡° ê²€ì‚¬ ë„êµ¬")
    parser.add_argument("--model_path", type=str, default="./llama-1b-civil-law-specialist",
                       help="ê²€ì‚¬í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--base_model", type=str, default="meta-llama/Llama-3.2-1B",
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ê²€ì‚¬
    inspect_saved_model(args.model_path)
    
    # í˜¸í™˜ì„± ê²€ì‚¬
    check_base_model_compatibility(args.base_model, args.model_path)
    
    # í•´ê²° ë°©ì•ˆ ì œì‹œ
    suggest_fixes(args.model_path)

if __name__ == "__main__":
    main()
