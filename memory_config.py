# memory_optimizer.py
# RTX 4060 8GB ë©”ëª¨ë¦¬ ìµœì í™” ë„êµ¬

import torch
import psutil
import os
import gc

class MemoryOptimizer:
    """RTX 4060 8GBë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.gpu_total = 8.0  # RTX 4060 8GB
        self.gpu_safe_limit = 6.5  # ì•ˆì „ ì—¬ìœ ë¶„ 1.5GB
        
    def check_memory_status(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
        status = {
            "cpu_percent": psutil.virtual_memory().percent,
            "cpu_available_gb": psutil.virtual_memory().available / 1024**3,
        }
        
        if torch.cuda.is_available():
            status.update({
                "gpu_allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "gpu_reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "gpu_available_gb": self.gpu_total - (torch.cuda.memory_allocated() / 1024**3),
            })
        
        return status
    
    def print_memory_status(self):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
        status = self.check_memory_status()
        
        print("=" * 50)
        print("ğŸ–¥ï¸  ë©”ëª¨ë¦¬ ìƒíƒœ")
        print("=" * 50)
        print(f"ğŸ’¾ CPU ë©”ëª¨ë¦¬: {status['cpu_percent']:.1f}% ì‚¬ìš©")
        print(f"ğŸ’¾ CPU ì—¬ìœ : {status['cpu_available_gb']:.2f}GB")
        
        if torch.cuda.is_available():
            print(f"ğŸ® GPU í• ë‹¹: {status['gpu_allocated_gb']:.2f}GB")
            print(f"ğŸ® GPU ì˜ˆì•½: {status['gpu_reserved_gb']:.2f}GB")
            print(f"ğŸ® GPU ì—¬ìœ : {status['gpu_available_gb']:.2f}GB")
            
            if status['gpu_available_gb'] < 2.0:
                print("âš ï¸  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!")
            elif status['gpu_available_gb'] < 3.0:
                print("âš ï¸  GPU ë©”ëª¨ë¦¬ ì—¬ìœ  ë¶€ì¡±")
            else:
                print("âœ… GPU ë©”ëª¨ë¦¬ ì—¬ìœ  ì¶©ë¶„")
        else:
            print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
        
        print("=" * 50)
    
    def aggressive_cleanup(self):
        """ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("ğŸ§¹ ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        print(f"ğŸ—‘ï¸  Python ê°ì²´ {collected}ê°œ ì •ë¦¬")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("ğŸ® GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
        self.set_memory_env_vars()
        
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def set_memory_env_vars(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
        memory_configs = {
            "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:128,expandable_segments:True",
            "OMP_NUM_THREADS": "4",
            "MKL_NUM_THREADS": "4",
            "TOKENIZERS_PARALLELISM": "false"
        }
        
        for key, value in memory_configs.items():
            os.environ[key] = value
            print(f"ğŸ”§ {key} = {value}")
    
    def can_load_model(self, model_size_gb=2.5):
        """ëª¨ë¸ ë¡œë”© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        status = self.check_memory_status()
        
        if not torch.cuda.is_available():
            return False, "CUDA ì‚¬ìš© ë¶ˆê°€"
        
        available = status.get('gpu_available_gb', 0)
        
        if available >= model_size_gb:
            return True, f"GPU ë¡œë”© ê°€ëŠ¥ (ì—¬ìœ : {available:.2f}GB)"
        else:
            return False, f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (í•„ìš”: {model_size_gb}GB, ì—¬ìœ : {available:.2f}GB)"
    
    def get_optimal_batch_size(self):
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ"""
        status = self.check_memory_status()
        available = status.get('gpu_available_gb', 0)
        
        if available >= 4.0:
            return 2, "ì—¬ìœ  ì¶©ë¶„"
        elif available >= 2.0:
            return 1, "ì ì • ìˆ˜ì¤€"
        else:
            return 1, "ìµœì†Œ ì„¤ì • (CPU ê¶Œì¥)"
    
    def force_cpu_mode(self):
        """CPU ëª¨ë“œ ê°•ì œ ì „í™˜"""
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        print("ğŸ”„ CPU ëª¨ë“œë¡œ ê°•ì œ ì „í™˜")
    
    def restore_gpu_mode(self):
        """GPU ëª¨ë“œ ë³µêµ¬"""
        if "CUDA_VISIBLE_DEVICES" in os.environ:
            del os.environ["CUDA_VISIBLE_DEVICES"]
        print("ğŸ”„ GPU ëª¨ë“œ ë³µêµ¬")

def check_rtx4060_compatibility():
    """RTX 4060 í˜¸í™˜ì„± ì²´í¬"""
    optimizer = MemoryOptimizer()
    
    print("ğŸš€ RTX 4060 8GB í˜¸í™˜ì„± ì²´í¬")
    optimizer.print_memory_status()
    
    # ëª¨ë¸ ë¡œë”© ì²´í¬
    can_load, msg = optimizer.can_load_model(2.5)
    print(f"ğŸ“Š ëª¨ë¸ ë¡œë”© ê°€ëŠ¥ì„±: {msg}")
    
    # ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
    batch_size, reason = optimizer.get_optimal_batch_size()
    print(f"ğŸ“ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {batch_size} ({reason})")
    
    # ìµœì í™” ê¶Œì¥ì‚¬í•­
    print("\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
    status = optimizer.check_memory_status()
    
    if status.get('gpu_available_gb', 0) < 2.0:
        print("   1. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
        print("   2. ì‹œìŠ¤í…œ ì¬ë¶€íŒ…")
        print("   3. CPU ëª¨ë“œ ì‚¬ìš©")
    elif status.get('gpu_available_gb', 0) < 3.0:
        print("   1. ultra_light í”„ë¦¬ì…‹ ì‚¬ìš©")
        print("   2. ë°°ì¹˜ í¬ê¸° 1ë¡œ ì œí•œ")
        print("   3. ë©”ëª¨ë¦¬ ì¡°ê°í™” ë°©ì§€ í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
    else:
        print("   âœ… í˜„ì¬ ìƒíƒœë¡œ í›ˆë ¨ ê°€ëŠ¥")

if __name__ == "__main__":
    check_rtx4060_compatibility()
