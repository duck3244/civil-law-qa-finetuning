# fixed_model_setup.py
# ì™„ì „íˆ ìˆ˜ì •ëœ ëª¨ë¸ ì„¤ì • (core/model_setup.py êµì²´ìš©)

import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import setup_chat_format
from peft import LoraConfig, get_peft_model, PeftModel
import json
from datetime import datetime


class ImprovedModelManager:
    """ê°œì„ ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, model_name="meta-llama/Llama-3.2-1B"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.peft_config = None
        self.model_info = {}

    def load_model_and_tokenizer(self,
                                 load_in_4bit=True,
                                 load_in_8bit=False,
                                 torch_dtype=torch.bfloat16,
                                 device_map="auto",
                                 trust_remote_code=True,
                                 use_cache=False,
                                 low_cpu_mem_usage=True,
                                 attn_implementation="eager"):
        """
        ê°œì„ ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        """
        print(f"=== ê°œì„ ëœ ëª¨ë¸ ë¡œë”©: {self.model_name} ===")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

        # ê°œì„ ëœ BitsAndBytesConfig ì„¤ì •
        quantization_config = None
        if load_in_4bit or load_in_8bit:
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=load_in_4bit,
                load_in_8bit=load_in_8bit,
                bnb_4bit_quant_type="nf4" if load_in_4bit else None,
                bnb_4bit_compute_dtype=torch_dtype if load_in_4bit else None,
                bnb_4bit_use_double_quant=True if load_in_4bit else False,
                bnb_4bit_quant_storage=torch_dtype if load_in_4bit else None,
            )
            print(f"ğŸ”§ ì–‘ìí™” ì„¤ì •: {'4bit' if load_in_4bit else '8bit'}")

        try:
            # ëª¨ë¸ ë¡œë“œ (ê°œì„ ëœ ì„¤ì •)
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "device_map": device_map,
                "trust_remote_code": trust_remote_code,
                "use_cache": use_cache,
                "low_cpu_mem_usage": low_cpu_mem_usage,
            }

            # ì–‘ìí™” ì„¤ì • ì¶”ê°€
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config

            # Attention êµ¬í˜„ ì„¤ì •
            if attn_implementation != "eager":
                model_kwargs["attn_implementation"] = attn_implementation
                print(f"ğŸ”§ Attention êµ¬í˜„: {attn_implementation}")

            # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (GPU í™˜ê²½ì—ì„œ)
            if torch.cuda.is_available() and device_map == "auto":
                model_kwargs["max_memory"] = {0: "6GB", "cpu": "8GB"}

            print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì„±ê³µ")

        except torch.cuda.OutOfMemoryError:
            print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. CPU ìš°ì„  ë¡œë”©ìœ¼ë¡œ ì¬ì‹œë„...")
            # CPU ìš°ì„  ë¡œë”© í›„ ë¶€ë¶„ì  GPU ì´ë™
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                device_map="cpu",
                quantization_config=quantization_config,
                trust_remote_code=trust_remote_code,
                use_cache=use_cache,
                low_cpu_mem_usage=True,
            )
            print("âœ… CPU ëª¨ë“œë¡œ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì„±ê³µ")

        except Exception as e:
            print(f"âš ï¸ ê¸°ë³¸ ë¡œë”© ì‹¤íŒ¨, ì•ˆì „ ëª¨ë“œë¡œ ì¬ì‹œë„: {e}")
            # ì•ˆì „ ëª¨ë“œ: ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¡œë”©
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=trust_remote_code,
                low_cpu_mem_usage=True
            )
            print("âœ… ì•ˆì „ ëª¨ë“œë¡œ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì„±ê³µ")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ (ê°œì„ ëœ ì„¤ì •)
        print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=trust_remote_code,
            padding_side="right",
            truncation_side="right",
            use_fast=True  # Fast tokenizer ì‚¬ìš©
        )

        # íŒ¨ë”© í† í° ì„¤ì • (ê°œì„ ëœ ë°©ë²•)
        if self.tokenizer.pad_token is None:
            # EOS í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ë³„ë„ ID í• ë‹¹
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")

        # ì±„íŒ… í¬ë§· ì„¤ì • (ê¸°ì¡´ í…œí”Œë¦¿ í™•ì¸ í›„ ì ìš©)
        if not hasattr(self.tokenizer, 'chat_template') or self.tokenizer.chat_template is None:
            print("ğŸ”§ Chat template ì„¤ì • ì¤‘...")
            try:
                self.model, self.tokenizer = setup_chat_format(
                    self.model,
                    self.tokenizer,
                    format="chatml",  # ëª…ì‹œì  í¬ë§· ì§€ì •
                    resize_to_multiple_of=8  # íš¨ìœ¨ì„±ì„ ìœ„í•œ í¬ê¸° ì¡°ì •
                )
                print("âœ… Chat template ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Chat template ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}")
        else:
            print("âœ… ê¸°ì¡´ chat template ì‚¬ìš©")

        # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        self._collect_model_info()

        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {self.model_info['num_parameters']:,}")
        print(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.model_info['vocab_size']:,}")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            memory_info = self.get_memory_usage()
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬: {memory_info['allocated_gb']:.2f}GB / {memory_info.get('reserved_gb', 'N/A')}GB")

        return self.model, self.tokenizer

    def _collect_model_info(self):
        """ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ (ê°œì„ ëœ ë²„ì „)"""
        if self.model and self.tokenizer:
            try:
                num_params = self.model.num_parameters()
            except:
                # ì–‘ìí™”ëœ ëª¨ë¸ì˜ ê²½ìš° ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
                num_params = sum(p.numel() for p in self.model.parameters())

            self.model_info = {
                "model_name": self.model_name,
                "num_parameters": num_params,
                "vocab_size": len(self.tokenizer),
                "model_type": self.model.config.model_type,
                "hidden_size": getattr(self.model.config, 'hidden_size', 'Unknown'),
                "num_layers": getattr(self.model.config, 'num_hidden_layers', 'Unknown'),
                "num_attention_heads": getattr(self.model.config, 'num_attention_heads', 'Unknown'),
                "torch_dtype": str(self.model.dtype) if hasattr(self.model, 'dtype') else 'Unknown',
                "device": str(self.model.device) if hasattr(self.model, 'device') else 'Unknown'
            }

    def setup_lora_config(self,
                          r=32,  # ì¦ê°€ëœ ê¸°ë³¸ê°’
                          lora_alpha=64,  # ì¦ê°€ëœ ê¸°ë³¸ê°’
                          lora_dropout=0.1,  # ì¦ê°€ëœ ê¸°ë³¸ê°’
                          target_modules=None,
                          modules_to_save=None,  # ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ None
                          task_type="CAUSAL_LM",
                          base_model_name_or_path=None):
        """
        ê°œì„ ëœ LoRA ì„¤ì • êµ¬ì„±
        """

        # base_model_name_or_path ëª…ì‹œì  ì„¤ì •
        if base_model_name_or_path is None:
            base_model_name_or_path = self.model_name

        # target_modules ìë™ ì„¤ì • (ëª¨ë¸ì— ë”°ë¼)
        if target_modules is None:
            if "llama" in self.model_name.lower():
                target_modules = [
                    "q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"
                ]
            else:
                target_modules = "all-linear"

        # modules_to_saveëŠ” Noneìœ¼ë¡œ ì„¤ì • (ë¬¸ì œ ë°©ì§€)
        # embed_tokensì™€ lm_head ì €ì¥ ì‹œ í‚¤ ë§¤í•‘ ë¬¸ì œ ë°œìƒ
        modules_to_save = None

        self.peft_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            modules_to_save=modules_to_save,
            task_type=task_type,
            base_model_name_or_path=base_model_name_or_path,  # ëª…ì‹œì  ì„¤ì •
            bias="none",  # bias í•™ìŠµ ì•ˆí•¨
            use_rslora=False,  # RSLoRA ë¹„í™œì„±í™”
            use_dora=False,  # DoRA ë¹„í™œì„±í™”
        )

        print(f"=== ê°œì„ ëœ LoRA ì„¤ì • ì™„ë£Œ ===")
        print(f"Rank (r): {r}")
        print(f"Alpha: {lora_alpha}")
        print(f"Dropout: {lora_dropout}")
        print(f"Target modules: {target_modules}")
        print(f"Base model: {base_model_name_or_path}")
        print(f"Modules to save: {modules_to_save}")

        return self.peft_config

    def apply_lora(self):
        """ëª¨ë¸ì— LoRA ì ìš© (ê°œì„ ëœ ë²„ì „)"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.peft_config is None:
            print("LoRA ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.setup_lora_config()

        try:
            print("ğŸ”— LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
            self.model = get_peft_model(self.model, self.peft_config)
            print("âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ LoRA ì ìš© ì¤‘ ì˜¤ë¥˜: {e}")
            print("ğŸ”„ ì•ˆì „ ëª¨ë“œë¡œ ì¬ì‹œë„...")

            # ë” ë³´ìˆ˜ì ì¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
            safe_config = LoraConfig(
                r=16,
                lora_alpha=32,
                lora_dropout=0.05,
                target_modules=["q_proj", "v_proj"],  # ìµœì†Œ ëª¨ë“ˆë§Œ
                modules_to_save=None,
                task_type="CAUSAL_LM",
                base_model_name_or_path=self.model_name
            )

            self.model = get_peft_model(self.model, safe_config)
            self.peft_config = safe_config
            print("âœ… ì•ˆì „ ëª¨ë“œë¡œ LoRA ì ìš© ì„±ê³µ")

        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())

        print(f"=== LoRA ì ìš© ì™„ë£Œ ===")
        print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")

        return self.model

    def get_model_size_info(self):
        """ëª¨ë¸ í¬ê¸° ì •ë³´ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        if self.model is None:
            return None

        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            param_size = total_params * 4 / (1024 ** 3)  # float32 ê¸°ì¤€ GB
            if hasattr(self.model, 'dtype'):
                if self.model.dtype == torch.float16 or self.model.dtype == torch.bfloat16:
                    param_size = total_params * 2 / (1024 ** 3)  # half precision

            return {
                "total_parameters": total_params,
                "trainable_parameters": trainable_params,
                "trainable_percentage": 100 * trainable_params / total_params if total_params > 0 else 0,
                "estimated_size_gb": param_size,
                "model_dtype": str(self.model.dtype) if hasattr(self.model, 'dtype') else 'Unknown'
            }
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ í¬ê¸° ì •ë³´ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None

    def save_model_config(self, output_dir):
        """ëª¨ë¸ ì„¤ì • ì •ë³´ ì €ì¥ (ê°œì„ ëœ ì•ˆì „ ë²„ì „)"""

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)

        def safe_json_convert(obj):
            """JSON ì§ë ¬í™” ì•ˆì „ ë³€í™˜"""
            if obj is None:
                return None
            elif isinstance(obj, (int, float, str, bool)):
                return obj
            elif isinstance(obj, (list, tuple)):
                return [safe_json_convert(item) for item in obj]
            elif isinstance(obj, dict):
                return {k: safe_json_convert(v) for k, v in obj.items()}
            elif hasattr(obj, '__dict__'):
                return safe_json_convert(obj.__dict__)
            else:
                return str(obj)

        try:
            # ì•ˆì „í•œ ì„¤ì • ì •ë³´ ìˆ˜ì§‘
            config_info = {
                "model_info": safe_json_convert(self.model_info),
                "size_info": safe_json_convert(self.get_model_size_info()),
                "model_name": self.model_name,
                "saved_at": datetime.now().isoformat(),
                "version": "2.0"
            }

            # LoRA ì„¤ì • ì•ˆì „í•˜ê²Œ ì¶”ê°€
            if self.peft_config:
                try:
                    lora_config_dict = {}
                    for key, value in self.peft_config.__dict__.items():
                        if key.startswith('_'):
                            continue
                        lora_config_dict[key] = safe_json_convert(value)

                    config_info["lora_config"] = lora_config_dict
                except Exception as lora_error:
                    print(f"âš ï¸ LoRA ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜: {lora_error}")
                    config_info["lora_config"] = {"error": str(lora_error)}

            # ì €ì¥
            config_path = f"{output_dir}/model_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, ensure_ascii=False, indent=2)

            print(f"âœ… ëª¨ë¸ ì„¤ì • ì €ì¥: {config_path}")
            return config_path

        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

            # ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ì €ì¥ ì‹œë„
            try:
                minimal_config = {
                    "model_name": self.model_name,
                    "error": str(e),
                    "saved_at": datetime.now().isoformat(),
                    "version": "2.0_minimal"
                }

                config_path = f"{output_dir}/model_config_minimal.json"
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(minimal_config, f, ensure_ascii=False, indent=2)

                print(f"ğŸ“ ìµœì†Œ ì„¤ì • ì €ì¥: {config_path}")
                return config_path

            except Exception as e2:
                print(f"âŒ ì„¤ì • ì €ì¥ ì™„ì „ ì‹¤íŒ¨: {e2}")
                return None

    def prepare_for_training(self):
        """í›ˆë ¨ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ (ê°œì„ ëœ ë²„ì „)"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            try:
                self.model.gradient_checkpointing_enable()
                print("âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”")
            except Exception as e:
                print(f"âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‹¤íŒ¨: {e}")

        # í›ˆë ¨ ëª¨ë“œ ì„¤ì •
        self.model.train()

        # LoRA ë ˆì´ì–´ë§Œ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì • (ì•ˆì „ì„±)
        if hasattr(self.model, 'peft_config'):
            for name, module in self.model.named_modules():
                if 'lora' in name.lower():
                    module.train()

        print("âœ… í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ")
        return self.model, self.tokenizer

    def get_memory_usage(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024 ** 3  # GB
            reserved = torch.cuda.memory_reserved() / 1024 ** 3  # GB
            max_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3

            return {
                "allocated_gb": round(allocated, 2),
                "reserved_gb": round(reserved, 2),
                "max_memory_gb": round(max_memory, 2),
                "utilization_percent": round((allocated / max_memory) * 100, 1),
                "device_count": torch.cuda.device_count(),
                "device_name": torch.cuda.get_device_name(0)
            }
        else:
            return {"message": "CUDA not available"}

    def load_finetuned_model(self, model_path):
        """Fine-tuned ëª¨ë¸ ë¡œë“œ (ê°œì„ ëœ ì•ˆì „ ë²„ì „)"""
        print(f"=== Fine-tuned ëª¨ë¸ ë¡œë”©: {model_path} ===")

        try:
            # ì„¤ì • íŒŒì¼ì—ì„œ ë² ì´ìŠ¤ ëª¨ë¸ í™•ì¸
            config_path = os.path.join(model_path, "adapter_config.json")
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    adapter_config = json.load(f)

                saved_base_model = adapter_config.get('base_model_name_or_path')
                if saved_base_model and saved_base_model != 'None':
                    self.model_name = saved_base_model
                    print(f"ğŸ“‚ ì„¤ì •ì—ì„œ ë² ì´ìŠ¤ ëª¨ë¸ í™•ì¸: {self.model_name}")

            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_4bit=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            model = PeftModel.from_pretrained(base_model, model_path)

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
            except:
                tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    trust_remote_code=True
                )

            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            self.model = model
            self.tokenizer = tokenizer

            print(f"âœ… Fine-tuned ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return model, tokenizer

        except Exception as e:
            print(f"âŒ Fine-tuned ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e


# ModelManager í´ë˜ìŠ¤ ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
ModelManager = ImprovedModelManager

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê°œì„ ëœ ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = ImprovedModelManager("meta-llama/Llama-3.2-1B")

    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = manager.load_model_and_tokenizer()

    # LoRA ì„¤ì • ë° ì ìš©
    peft_config = manager.setup_lora_config()
    model = manager.apply_lora()

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    memory_info = manager.get_memory_usage()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info}")

    # ëª¨ë¸ ì„¤ì • ì €ì¥
    manager.save_model_config("./model_output")