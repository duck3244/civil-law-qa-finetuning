# model_setup.py
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì • ë‹´ë‹¹

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import setup_chat_format
from peft import LoraConfig, get_peft_model, PeftModel
import json
from datetime import datetime

class ModelManager:
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name="meta-llama/Llama-3.2-1B"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.peft_config = None
        self.model_info = {}
    
    def load_model_and_tokenizer(self, 
                                 load_in_4bit=True,
                                 load_in_8bit=False,
                                 torch_dtype=torch.bfloat16,
                                 device_map="auto",
                                 trust_remote_code=True,
                                 use_cache=False,
                                 low_cpu_mem_usage=True):
        """
        ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        """
        print(f"=== ëª¨ë¸ ë¡œë”©: {self.model_name} ===")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        # BitsAndBytesConfig ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™”)
        quantization_config = None
        if load_in_4bit or load_in_8bit:
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=load_in_4bit,
                load_in_8bit=load_in_8bit,
                bnb_4bit_quant_type="nf4" if load_in_4bit else None,
                bnb_4bit_compute_dtype=torch_dtype if load_in_4bit else None,
                bnb_4bit_use_double_quant=True if load_in_4bit else False,
                bnb_4bit_quant_storage=torch_dtype if load_in_4bit else None,
            )
        
        try:
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                device_map=device_map,
                quantization_config=quantization_config,
                trust_remote_code=trust_remote_code,
                use_cache=use_cache,
                low_cpu_mem_usage=low_cpu_mem_usage,
                max_memory={0: "6GB", "cpu": "8GB"},  # GPU ë©”ëª¨ë¦¬ ì œí•œ
            )
        except torch.cuda.OutOfMemoryError:
            print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. CPU ë©”ëª¨ë¦¬ ìš°ì„  ë¡œë”©ìœ¼ë¡œ ì¬ì‹œë„...")
            # CPU ìš°ì„  ë¡œë”© í›„ GPUë¡œ ì´ë™
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                device_map={"": "cpu"},  # ë¨¼ì € CPUì— ë¡œë“œ
                quantization_config=quantization_config,
                trust_remote_code=trust_remote_code,
                use_cache=use_cache,
                low_cpu_mem_usage=True,
            )
            # ë¶€ë¶„ì ìœ¼ë¡œ GPUë¡œ ì´ë™
            self.model = self.model.to("cuda:0")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=trust_remote_code,
            padding_side="right"
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ì±„íŒ… í¬ë§· ì„¤ì • (ê¸°ì¡´ í…œí”Œë¦¿ í™•ì¸)
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template is not None:
            print("âœ… ëª¨ë¸ì— ì´ë¯¸ chat templateì´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print("ğŸ”§ Chat templateì„ ì„¤ì •í•©ë‹ˆë‹¤.")
            try:
                self.model, self.tokenizer = setup_chat_format(self.model, self.tokenizer)
            except ValueError as e:
                if "Chat template is already added" in str(e):
                    print("âš ï¸ Chat templateì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.tokenizer.chat_template = None
                    self.model, self.tokenizer = setup_chat_format(self.model, self.tokenizer)
                else:
                    raise e
        
        # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        self._collect_model_info()
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {self.model_info['num_parameters']:,}")
        print(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.model_info['vocab_size']:,}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            memory_info = self.get_memory_usage()
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info['allocated_gb']:.2f}GB / {memory_info.get('total_gb', 'N/A')}GB")
        
        return self.model, self.tokenizer
        
        # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        self._collect_model_info()
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {self.model_info['num_parameters']:,}")
        print(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {self.model_info['vocab_size']:,}")
        
        return self.model, self.tokenizer
    
    def _collect_model_info(self):
        """ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘"""
        if self.model and self.tokenizer:
            self.model_info = {
                "model_name": self.model_name,
                "num_parameters": self.model.num_parameters(),
                "vocab_size": len(self.tokenizer),
                "model_type": self.model.config.model_type,
                "hidden_size": getattr(self.model.config, 'hidden_size', 'Unknown'),
                "num_layers": getattr(self.model.config, 'num_hidden_layers', 'Unknown'),
                "num_attention_heads": getattr(self.model.config, 'num_attention_heads', 'Unknown')
            }
    
    def setup_lora_config(self, 
                          r=16,
                          lora_alpha=32,
                          lora_dropout=0.05,
                          target_modules="all-linear",
                          modules_to_save=None,
                          task_type="CAUSAL_LM"):
        """
        LoRA ì„¤ì • êµ¬ì„±
        
        Args:
            r: LoRA rank
            lora_alpha: LoRA alpha íŒŒë¼ë¯¸í„°
            lora_dropout: LoRA dropout ë¹„ìœ¨
            target_modules: íƒ€ê²Ÿ ëª¨ë“ˆ
            modules_to_save: ì €ì¥í•  ëª¨ë“ˆ (íŠ¹ìˆ˜ í† í°ìš©)
            task_type: íƒœìŠ¤í¬ íƒ€ì…
        """
        if modules_to_save is None:
            modules_to_save = ["lm_head", "embed_tokens"]
        
        self.peft_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            modules_to_save=modules_to_save,
            task_type=task_type,
        )
        
        print(f"=== LoRA ì„¤ì • ì™„ë£Œ ===")
        print(f"Rank: {r}")
        print(f"Alpha: {lora_alpha}")
        print(f"Dropout: {lora_dropout}")
        print(f"Target modules: {target_modules}")
        
        return self.peft_config
    
    def apply_lora(self):
        """ëª¨ë¸ì— LoRA ì ìš©"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.peft_config is None:
            print("LoRA ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.setup_lora_config()
        
        self.model = get_peft_model(self.model, self.peft_config)
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"=== LoRA ì ìš© ì™„ë£Œ ===")
        print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        
        return self.model
    
    def get_model_size_info(self):
        """ëª¨ë¸ í¬ê¸° ì •ë³´ ë°˜í™˜"""
        if self.model is None:
            return None
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "trainable_percentage": 100 * trainable_params / total_params if total_params > 0 else 0
        }
    
    def save_model_config(self, output_dir):
        """ëª¨ë¸ ì„¤ì • ì •ë³´ ì €ì¥ (JSON ì§ë ¬í™” ì•ˆì „)"""
        def convert_to_serializable(obj):
            """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
            if isinstance(obj, set):
                return list(obj)
            elif hasattr(obj, '__dict__'):
                return str(obj)
            elif hasattr(obj, '__class__'):
                return str(obj.__class__.__name__)
            else:
                return str(obj)
        
        def safe_config_extraction(config):
            """ì•ˆì „í•œ ì„¤ì • ì¶”ì¶œ"""
            if config is None:
                return None
            
            safe_config = {}
            for key, value in config.__dict__.items():
                try:
                    # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                    json.dumps(value)
                    safe_config[key] = value
                except (TypeError, ValueError):
                    # ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì•ˆì „í•œ í˜•íƒœë¡œ ë³€í™˜
                    safe_config[key] = convert_to_serializable(value)
            
            return safe_config
        
        try:
            config_info = {
                "model_info": self.model_info,
                "size_info": self.get_model_size_info(),
                "lora_config": safe_config_extraction(self.peft_config) if self.peft_config else None,
                "model_name": self.model_name,
                "saved_at": datetime.now().isoformat()
            }
            
            config_path = f"{output_dir}/model_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, ensure_ascii=False, indent=2)
            
            print(f"ëª¨ë¸ ì„¤ì • ì €ì¥: {config_path}")
            return config_path
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            # ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ì €ì¥
            try:
                minimal_config = {
                    "model_name": self.model_name,
                    "error": str(e),
                    "saved_at": datetime.now().isoformat()
                }
                config_path = f"{output_dir}/model_config_minimal.json"
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(minimal_config, f, ensure_ascii=False, indent=2)
                print(f"ìµœì†Œ ì„¤ì • ì €ì¥: {config_path}")
                return config_path
            except Exception as e2:
                print(f"âŒ ì„¤ì • ì €ì¥ ì™„ì „ ì‹¤íŒ¨: {e2}")
                return None
    
    def load_finetuned_model(self, model_path):
        """Fine-tuned ëª¨ë¸ ë¡œë“œ"""
        print(f"=== Fine-tuned ëª¨ë¸ ë¡œë”©: {model_path} ===")
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_4bit=True,
            trust_remote_code=True
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        print(f"âœ… Fine-tuned ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        return model, tokenizer
    
    def prepare_for_training(self):
        """í›ˆë ¨ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
        
        # í›ˆë ¨ ëª¨ë“œ ì„¤ì •
        self.model.train()
        
        print("âœ… í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ")
        
        return self.model, self.tokenizer
    
    def get_memory_usage(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            
            return {
                "allocated_gb": round(allocated, 2),
                "reserved_gb": round(reserved, 2),
                "device_count": torch.cuda.device_count()
            }
        else:
            return {"message": "CUDA not available"}

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = ModelManager("meta-llama/Llama-3.2-1B")
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = manager.load_model_and_tokenizer()
    
    # LoRA ì„¤ì • ë° ì ìš©
    peft_config = manager.setup_lora_config()
    model = manager.apply_lora()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    memory_info = manager.get_memory_usage()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info}")
    
    # ëª¨ë¸ ì„¤ì • ì €ì¥
    manager.save_model_config("./model_output")