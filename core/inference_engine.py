# inference_engine.py
# ì¶”ë¡  ë° í…ŒìŠ¤íŠ¸ ì—”ì§„

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import time
from datetime import datetime
import os

class InferenceEngine:
    """ë¯¼ë²• ì „ë¬¸ê°€ ëª¨ë¸ ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self, model=None, tokenizer=None):
        self.model = model
        self.tokenizer = tokenizer
        self.generation_config = self._get_default_generation_config()
        self.system_prompts = self._get_system_prompts()
    
    def _get_default_generation_config(self):
        """ê¸°ë³¸ ìƒì„± ì„¤ì •"""
        return {
            "max_new_tokens": 400,
            "do_sample": True,
            "temperature": 0.3,  # ë²•ë¥  ë‹µë³€ì´ë¯€ë¡œ ë‚®ì€ ì˜¨ë„
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "pad_token_id": None  # í† í¬ë‚˜ì´ì € ë¡œë“œ í›„ ì„¤ì •
        }
    
    def _get_system_prompts(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return {
            "ì „ì„¸ì‚¬ê¸°": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ì „ì„¸ì‚¬ê¸° ë° ì„ëŒ€ì°¨ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì „ì„¸ ê´€ë ¨ ë²•ë¥  ë¬¸ì œì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "ë¶€ë™ì‚°ë¬¼ê¶Œ": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶€ë™ì‚°ë¬¼ê¶Œ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì†Œìœ ê¶Œ, ë‹´ë³´ê¶Œ, ìš©ìµë¬¼ê¶Œ ë“±ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì„¸í•œ ë²•ë¥  ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "default": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¯¼ë²• ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
    
    def load_finetuned_model(self, model_path, base_model_name="meta-llama/Llama-3.2-1B"):
        """Fine-tuned ëª¨ë¸ ë¡œë“œ (í‚¤ ë§¤í•‘ ë¬¸ì œ í•´ê²°)"""
        print(f"=== Fine-tuned ëª¨ë¸ ë¡œë”©: {model_path} ===")
        
        # ì €ì¥ëœ ì„¤ì •ì—ì„œ ì›ë³¸ ë² ì´ìŠ¤ ëª¨ë¸ í™•ì¸
        config_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                adapter_config = json.load(f)
            
            saved_base_model = adapter_config.get('base_model_name_or_path', base_model_name)
            print(f"ğŸ“‚ ì €ì¥ëœ ë² ì´ìŠ¤ ëª¨ë¸: {saved_base_model}")
            
            # ì €ì¥ëœ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê²½ê³ 
            if base_model_name != saved_base_model:
                print(f"âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ë¶ˆì¼ì¹˜ ê°ì§€!")
                print(f"   ìš”ì²­ëœ ëª¨ë¸: {base_model_name}")
                print(f"   ì €ì¥ëœ ëª¨ë¸: {saved_base_model}")
                print(f"ğŸ”„ ì €ì¥ëœ ëª¨ë¸ë¡œ ë³€ê²½í•˜ì—¬ ì‹œë„...")
                base_model_name = saved_base_model
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            allocated = torch.cuda.memory_allocated() / 1024**3
            available = torch.cuda.get_device_properties(0).total_memory / 1024**3 - allocated
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: ì‚¬ìš© {allocated:.2f}GB, ì—¬ìœ  {available:.2f}GB")
            
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ëª¨ë“œë¡œ ì „í™˜
            if available < 2.0:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return self._load_cpu_mode(model_path, base_model_name)
        
        try:
            # GPU ë©”ëª¨ë¦¬ ì œí•œì  ë¡œë”© ì‹œë„
            print("ğŸ”„ GPU ë©”ëª¨ë¦¬ ì œí•œì  ë¡œë”© ì‹œë„...")
            
            # ì•ˆì „í•œ ì–‘ìí™” ì„¤ì •
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=False,
            )
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ì •í™•í•œ ëª¨ë¸ëª… ì‚¬ìš©)
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,  # ì €ì¥ëœ ì„¤ì •ì˜ ëª¨ë¸ëª… ì‚¬ìš©
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                max_memory={0: "3GB", "cpu": "16GB"},
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ (ì•ˆì „ ëª¨ë“œ)
            print("ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë”©...")
            try:
                self.model = PeftModel.from_pretrained(
                    base_model, 
                    model_path,
                    is_trainable=False  # ì¶”ë¡  ëª¨ë“œë¡œ ëª…ì‹œ
                )
            except KeyError as key_error:
                print(f"âš ï¸ í‚¤ ë§¤í•‘ ì˜¤ë¥˜ ê°ì§€: {key_error}")
                print("ğŸ”„ ëŒ€ì•ˆ ë¡œë”© ë°©ë²• ì‹œë„...")
                
                # ëŒ€ì•ˆ 1: ì–´ëŒ‘í„°ë¥¼ ì§ì ‘ ë¡œë“œ
                from peft import LoraConfig
                
                # ì €ì¥ëœ LoRA ì„¤ì • ë¡œë“œ
                lora_config = LoraConfig.from_pretrained(model_path)
                
                # ìƒˆë¡œìš´ PEFT ëª¨ë¸ ìƒì„± í›„ ê°€ì¤‘ì¹˜ ë¡œë“œ
                from peft import get_peft_model
                self.model = get_peft_model(base_model, lora_config)
                
                # ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ
                import safetensors
                adapter_weights_path = os.path.join(model_path, "adapter_model.safetensors")
                if os.path.exists(adapter_weights_path):
                    adapter_weights = safetensors.torch.load_file(adapter_weights_path)
                    self.model.load_state_dict(adapter_weights, strict=False)
                    print("âœ… ëŒ€ì•ˆ ë¡œë”© ì„±ê³µ")
                else:
                    raise Exception("ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            self.model.eval()
            
        except Exception as gpu_error:
            print(f"âŒ GPU ë¡œë”© ì‹¤íŒ¨: {gpu_error}")
            print("ğŸ”„ CPU ëª¨ë“œë¡œ fallback...")
            return self._load_cpu_mode(model_path, base_model_name)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,  # ì €ì¥ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                trust_remote_code=True
            )
        except:
            # ì €ì¥ëœ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            print("âš ï¸ ì €ì¥ëœ í† í¬ë‚˜ì´ì €ê°€ ì—†ìŠµë‹ˆë‹¤. ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_name,
                trust_remote_code=True
            )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
        
        print(f"âœ… GPU ëª¨ë“œ ë¡œë”© ì™„ë£Œ")
        print(f"ğŸ“Š ëª¨ë¸ ë°ì´í„° íƒ€ì…: {next(self.model.parameters()).dtype}")
        print(f"ğŸ–¥ï¸ ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(self.model.parameters()).device}")
        
        return self.model, self.tokenizer
    
    def _load_cpu_mode(self, model_path, base_model_name):
        """CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # CPU ì „ìš© ë¡œë”©
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            self.model = PeftModel.from_pretrained(base_model, model_path)
            self.model.eval()
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
            
            print(f"âœ… CPU ëª¨ë“œ ë¡œë”© ì™„ë£Œ")
            print(f"âš ï¸ CPU ëª¨ë“œëŠ” ì¶”ë¡  ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return self.model, self.tokenizer
            
        except Exception as e:
            print(f"âŒ CPU ëª¨ë“œ ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
            raise e
    
    def generate_answer(self, question, category="default", **kwargs):
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
            category: ì¹´í…Œê³ ë¦¬ ("ì „ì„¸ì‚¬ê¸°", "ë¶€ë™ì‚°ë¬¼ê¶Œ", "default")
            **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
        """
        if self.model is None or self.tokenizer is None:
            raise ValueError("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        system_content = self.system_prompts.get(category, self.system_prompts["default"])
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": question}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§• (ë””ë°”ì´ìŠ¤ì™€ ë°ì´í„° íƒ€ì… ì¼ì¹˜)
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        device = next(self.model.parameters()).device
        dtype = next(self.model.parameters()).dtype
        
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        generation_config = self.generation_config.copy()
        generation_config.update(kwargs)
        
        # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        
        # ë‹µë³€ ìƒì„±
        start_time = time.time()
        
        try:
            with torch.no_grad():
                # use_cacheë¥¼ Falseë¡œ ì„¤ì •í•˜ì—¬ gradient checkpointing ë¬¸ì œ í•´ê²°
                outputs = self.model.generate(
                    **inputs,
                    use_cache=False,  # gradient checkpointingê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
                    **generation_config
                )
        except RuntimeError as e:
            if "dtype" in str(e) or "float" in str(e):
                print("âš ï¸ ë°ì´í„° íƒ€ì… ë¬¸ì œ ê°ì§€. ëª¨ë¸ì„ float32ë¡œ ë³€í™˜ í›„ ì¬ì‹œë„...")
                # ëª¨ë¸ì„ float32ë¡œ ë³€í™˜í•˜ì—¬ ì¬ì‹œë„
                self.model = self.model.float()
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        use_cache=False,
                        **generation_config
                    )
            else:
                raise e
        
        generation_time = time.time() - start_time
        
        # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œê±°)
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return {
            "answer": response.strip(),
            "generation_time": generation_time,
            "tokens_generated": len(outputs[0]) - len(inputs.input_ids[0])
        }
    
    def batch_generate(self, questions, categories=None, **kwargs):
        """
        ë°°ì¹˜ ì¶”ë¡ 
        
        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            categories: ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
        """
        if categories is None:
            categories = ["default"] * len(questions)
        
        if len(questions) != len(categories):
            raise ValueError("ì§ˆë¬¸ ìˆ˜ì™€ ì¹´í…Œê³ ë¦¬ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        results = []
        total_time = 0
        
        for i, (question, category) in enumerate(zip(questions, categories)):
            print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(questions)}")
            
            result = self.generate_answer(question, category, **kwargs)
            results.append({
                "question": question,
                "category": category,
                **result
            })
            total_time += result["generation_time"]
        
        return {
            "results": results,
            "total_time": total_time,
            "average_time": total_time / len(questions)
        }
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ"""
        print("=== ë¯¼ë²• ì „ë¬¸ê°€ ì±„íŒ… ëª¨ë“œ ===")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì¹´í…Œê³ ë¦¬ë¥¼ ì§€ì •í•˜ë ¤ë©´ 'ì¹´í…Œê³ ë¦¬:ì§ˆë¬¸' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì˜ˆ: ì „ì„¸ì‚¬ê¸°:ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì§€ ëª»í–ˆì–´ìš”")
        print("-" * 50)
        
        chat_history = []
        
        while True:
            user_input = input("\nì§ˆë¬¸: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                continue
            
            # ì¹´í…Œê³ ë¦¬ íŒŒì‹±
            if ':' in user_input:
                category, question = user_input.split(':', 1)
                category = category.strip()
                question = question.strip()
            else:
                category = "default"
                question = user_input
            
            try:
                result = self.generate_answer(question, category)
                
                print(f"\n[{category}] ì „ë¬¸ê°€ ë‹µë³€:")
                print(result["answer"])
                print(f"\n(ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ)")
                
                # ì±„íŒ… ê¸°ë¡ ì €ì¥
                chat_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "question": question,
                    "category": category,
                    "answer": result["answer"],
                    "generation_time": result["generation_time"]
                })
                
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return chat_history
    
    def evaluate_on_test_cases(self, test_cases):
        """
        í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•œ í‰ê°€
        
        Args:
            test_cases: [{"question": str, "category": str, "expected": str (ì„ íƒ)}, ...]
        """
        print("=== í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€ ===")
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            question = test_case["question"]
            category = test_case.get("category", "default")
            expected = test_case.get("expected", None)
            
            print(f"\n{i}. [{category}] í…ŒìŠ¤íŠ¸")
            print(f"ì§ˆë¬¸: {question}")
            
            result = self.generate_answer(question, category)
            
            print(f"ìƒì„±ëœ ë‹µë³€: {result['answer']}")
            
            evaluation = {
                "test_id": i,
                "question": question,
                "category": category,
                "generated_answer": result["answer"],
                "generation_time": result["generation_time"],
                "tokens_generated": result["tokens_generated"]
            }
            
            if expected:
                evaluation["expected_answer"] = expected
                # ê°„ë‹¨í•œ ìœ ì‚¬ì„± í‰ê°€ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ ë©”íŠ¸ë¦­ ì‚¬ìš©)
                similarity = self._calculate_similarity(result["answer"], expected)
                evaluation["similarity_score"] = similarity
            
            results.append(evaluation)
            print("-" * 80)
        
        return results
    
    def _calculate_similarity(self, generated, expected):
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ê³„ì‚° (BLEU ìŠ¤ì½”ì–´ ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ì„±
        gen_words = set(generated.lower().split())
        exp_words = set(expected.lower().split())
        
        if not exp_words:
            return 0.0
        
        intersection = gen_words.intersection(exp_words)
        return len(intersection) / len(exp_words)
    
    def get_default_test_cases(self):
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë°˜í™˜"""
        return [
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ì „ì„¸ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì§€ ëª»í•  ê²ƒ ê°™ì€ë°, ì–´ë–¤ ë²•ì  ì ˆì°¨ë¥¼ ë°Ÿì„ ìˆ˜ ìˆë‚˜ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ", 
                "question": "ê·¼ì €ë‹¹ê¶Œì´ ì„¤ì •ëœ ë¶€ë™ì‚°ì„ ë§¤ìˆ˜í•  ë•Œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            },
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ì „ì„¸ ê³„ì•½ ì „ì— ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  ì„œë¥˜ë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ë¶€ë™ì‚° ì·¨ë“ì‹œíš¨ê°€ ì™„ì„±ë˜ë ¤ë©´ ì–´ë–¤ ì¡°ê±´ë“¤ì´ í•„ìš”í•œê°€ìš”?",
            },
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ê¹¡í†µì „ì„¸ë¥¼ í”¼í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ì „ì„¸ê¶Œê³¼ ì„ì°¨ê¶Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            }
        ]
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=== ë¯¼ë²• ì „ë¬¸ê°€ ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸ ===")
        
        test_cases = self.get_default_test_cases()
        results = self.evaluate_on_test_cases(test_cases)
        
        # í†µê³„ ê³„ì‚°
        total_time = sum(r["generation_time"] for r in results)
        avg_time = total_time / len(results)
        total_tokens = sum(r["tokens_generated"] for r in results)
        avg_tokens = total_tokens / len(results)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for result in results:
            category = result["category"]
            if category not in category_stats:
                category_stats[category] = {"count": 0, "total_time": 0, "total_tokens": 0}
            
            category_stats[category]["count"] += 1
            category_stats[category]["total_time"] += result["generation_time"]
            category_stats[category]["total_tokens"] += result["tokens_generated"]
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            "test_results": results,
            "overall_stats": {
                "total_tests": len(results),
                "total_time": total_time,
                "average_time": avg_time,
                "total_tokens": total_tokens,
                "average_tokens": avg_tokens,
                "tokens_per_second": total_tokens / total_time if total_time > 0 else 0
            },
            "category_stats": {
                cat: {
                    "count": stats["count"],
                    "avg_time": stats["total_time"] / stats["count"],
                    "avg_tokens": stats["total_tokens"] / stats["count"]
                }
                for cat, stats in category_stats.items()
            },
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"\n=== í…ŒìŠ¤íŠ¸ ìš”ì•½ ===")
        print(f"ì´ í…ŒìŠ¤íŠ¸: {summary['overall_stats']['total_tests']}ê°œ")
        print(f"í‰ê·  ìƒì„± ì‹œê°„: {summary['overall_stats']['average_time']:.2f}ì´ˆ")
        print(f"í‰ê·  í† í° ìˆ˜: {summary['overall_stats']['average_tokens']:.1f}ê°œ")
        print(f"ìƒì„± ì†ë„: {summary['overall_stats']['tokens_per_second']:.1f} í† í°/ì´ˆ")
        
        return summary
    
    def save_test_results(self, results, output_path):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path
    
    def benchmark_generation_speed(self, num_tests=10):
        """ìƒì„± ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        print(f"=== ìƒì„± ì†ë„ ë²¤ì¹˜ë§ˆí¬ ({num_tests}íšŒ í…ŒìŠ¤íŠ¸) ===")
        
        test_question = "ì „ì„¸ ê³„ì•½ì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        times = []
        token_counts = []
        
        for i in range(num_tests):
            print(f"í…ŒìŠ¤íŠ¸ {i+1}/{num_tests}")
            result = self.generate_answer(test_question, "ì „ì„¸ì‚¬ê¸°")
            times.append(result["generation_time"])
            token_counts.append(result["tokens_generated"])
        
        avg_time = sum(times) / len(times)
        avg_tokens = sum(token_counts) / len(token_counts)
        avg_speed = avg_tokens / avg_time if avg_time > 0 else 0
        
        benchmark_result = {
            "num_tests": num_tests,
            "times": times,
            "token_counts": token_counts,
            "average_time": avg_time,
            "average_tokens": avg_tokens,
            "average_speed_tokens_per_sec": avg_speed,
            "min_time": min(times),
            "max_time": max(times)
        }
        
        print(f"í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ")
        print(f"í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
        print(f"í‰ê·  ì†ë„: {avg_speed:.1f} í† í°/ì´ˆ")
        print(f"ìµœì†Œ/ìµœëŒ€ ì‹œê°„: {min(times):.2f}ì´ˆ / {max(times):.2f}ì´ˆ")
        
        return benchmark_result
    
    def update_generation_config(self, **kwargs):
        """ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.generation_config.update(kwargs)
        print(f"ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸: {kwargs}")
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model is None:
            return None
        
        info = {
            "model_type": type(self.model).__name__,
            "device": str(self.model.device) if hasattr(self.model, 'device') else "Unknown",
            "dtype": str(self.model.dtype) if hasattr(self.model, 'dtype') else "Unknown",
        }
        
        if hasattr(self.model, 'peft_config'):
            info["peft_type"] = "LoRA"
            info["trainable_params"] = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return info

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    engine = InferenceEngine()
    
    # Fine-tuned ëª¨ë¸ ë¡œë“œ
    model_path = "./llama-1b-civil-law-specialist"
    engine.load_finetuned_model(model_path)
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = engine.run_comprehensive_test()
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
    engine.save_test_results(test_results, "test_results.json")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_results = engine.benchmark_generation_speed()
    
    # ëŒ€í™”í˜• ëª¨ë“œ (ì„ íƒì‚¬í•­)
    # chat_history = engine.interactive_chat()
    
    print("\n=== ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
    print("ì‚¬ìš© ë°©ë²•:")
    print("1. engine.generate_answer('ì§ˆë¬¸', 'ì¹´í…Œê³ ë¦¬')")
    print("2. engine.interactive_chat()")
    print("3. engine.run_comprehensive_test()")