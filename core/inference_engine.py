# inference_engine.py
# ì¶”ë¡  ë° í…ŒìŠ¤íŠ¸ ì—”ì§„

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import time
from datetime import datetime
import os

class InferenceEngine:
    """ë¯¼ë²• ì „ë¬¸ê°€ ëª¨ë¸ ì¶”ë¡  ì—”ì§„"""

    def __init__(self, model=None, tokenizer=None):
        self.model = model
        self.tokenizer = tokenizer
        self.generation_config = self._get_default_generation_config()
        self.system_prompts = self._get_system_prompts()

    def _get_default_generation_config(self):
        """ê¸°ë³¸ ìƒì„± ì„¤ì •"""
        return {
            "max_new_tokens": 400,
            "do_sample": True,
            "temperature": 0.3,  # ë²•ë¥  ë‹µë³€ì´ë¯€ë¡œ ë‚®ì€ ì˜¨ë„
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "pad_token_id": None  # í† í¬ë‚˜ì´ì € ë¡œë“œ í›„ ì„¤ì •
        }

    def _get_system_prompts(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return {
            "ì „ì„¸ì‚¬ê¸°": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ì „ì„¸ì‚¬ê¸° ë° ì„ëŒ€ì°¨ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì „ì„¸ ê´€ë ¨ ë²•ë¥  ë¬¸ì œì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "ë¶€ë™ì‚°ë¬¼ê¶Œ": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶€ë™ì‚°ë¬¼ê¶Œ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì†Œìœ ê¶Œ, ë‹´ë³´ê¶Œ, ìš©ìµë¬¼ê¶Œ ë“±ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì„¸í•œ ë²•ë¥  ìƒë‹´ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "default": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¯¼ë²• ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }

    def load_finetuned_model(self, model_path, base_model_name="meta-llama/Llama-3.2-1B"):
        """ì™„ì „ ìˆ˜ì •ëœ Fine-tuned ëª¨ë¸ ë¡œë“œ ë©”ì„œë“œ"""
        print(f"=== ìˆ˜ì •ëœ ëª¨ë¸ ë¡œë”©: {model_path} ===")

        # ê°•ì œë¡œ ë² ì´ìŠ¤ ëª¨ë¸ ì§€ì • (ì €ì¥ëœ ì„¤ì • ë¬´ì‹œ)
        print(f"ğŸ”§ ê°•ì œ ë² ì´ìŠ¤ ëª¨ë¸: {base_model_name}")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

        try:
            # 1ë‹¨ê³„: ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©...")

            try:
                # GPU ë¡œë”© ì‹œë„
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )

                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                device_type = "GPU"

            except Exception as gpu_error:
                print(f"âš ï¸ GPU ë¡œë”© ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ì „í™˜: {gpu_error}")

                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                device_type = "CPU"

            print(f"âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì„±ê³µ ({device_type})")

            # 2ë‹¨ê³„: PEFT ì–´ëŒ‘í„° ë¡œë“œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            print("ğŸ”— PEFT ì–´ëŒ‘í„° ë¡œë”©...")

            # ë°©ë²• 1: ì§ì ‘ PeftModel ë¡œë“œ
            try:
                self.model = PeftModel.from_pretrained(
                    base_model,
                    model_path,
                    is_trainable=False
                )
                print("âœ… ë°©ë²• 1 ì„±ê³µ: ì§ì ‘ PeftModel ë¡œë“œ")

            except Exception as peft_error:
                print(f"âš ï¸ ë°©ë²• 1 ì‹¤íŒ¨: {peft_error}")

                # ë°©ë²• 2: ì„¤ì • íŒŒì¼ ì„ì‹œ ìˆ˜ì • í›„ ë¡œë“œ
                try:
                    print("ğŸ”„ ë°©ë²• 2: ì„¤ì • íŒŒì¼ ì„ì‹œ ìˆ˜ì •")

                    import json
                    import os

                    config_path = os.path.join(model_path, "adapter_config.json")

                    # ì›ë³¸ ì„¤ì • ë°±ì—…
                    with open(config_path, 'r') as f:
                        original_config = json.load(f)

                    # ì„ì‹œ ìˆ˜ì •
                    fixed_config = original_config.copy()
                    fixed_config['base_model_name_or_path'] = base_model_name

                    with open(config_path, 'w') as f:
                        json.dump(fixed_config, f, indent=2)

                    # ë¡œë“œ ì‹œë„
                    self.model = PeftModel.from_pretrained(
                        base_model,
                        model_path,
                        is_trainable=False
                    )

                    print("âœ… ë°©ë²• 2 ì„±ê³µ: ì„¤ì • íŒŒì¼ ìˆ˜ì • í›„ ë¡œë“œ")

                    # ì›ë³¸ ì„¤ì • ë³µêµ¬ (ì„ íƒì‚¬í•­)
                    # with open(config_path, 'w') as f:
                    #     json.dump(original_config, f, indent=2)

                except Exception as config_error:
                    print(f"âš ï¸ ë°©ë²• 2ë„ ì‹¤íŒ¨: {config_error}")

                    # ë°©ë²• 3: ìˆ˜ë™ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ë¡œë“œ
                    try:
                        print("ğŸ”„ ë°©ë²• 3: ìˆ˜ë™ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ë¡œë“œ")

                        from peft import LoraConfig, get_peft_model
                        import safetensors

                        # LoRA ì„¤ì • ë¡œë“œ ë° ì •ë¦¬
                        lora_config_dict = original_config.copy()

                        # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” í•„ë“œ ì œê±°
                        problematic_keys = ['base_model_name_or_path', 'revision']
                        for key in problematic_keys:
                            if key in lora_config_dict:
                                del lora_config_dict[key]

                        # LoRA ì„¤ì • ìƒì„±
                        lora_config = LoraConfig(**lora_config_dict)

                        # PEFT ëª¨ë¸ ìƒì„±
                        self.model = get_peft_model(base_model, lora_config)

                        # ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ
                        adapter_weights_path = os.path.join(model_path, "adapter_model.safetensors")

                        if os.path.exists(adapter_weights_path):
                            adapter_weights = safetensors.torch.load_file(adapter_weights_path, device="cpu")

                            # í‚¤ ì´ë¦„ ì •ë¦¬ ë° ë§¤í•‘
                            clean_weights = {}
                            for key, value in adapter_weights.items():
                                # ì¤‘ë³µëœ 'base_model' ì œê±°
                                if key.startswith('base_model.model.base_model.model.'):
                                    clean_key = key.replace('base_model.model.base_model.model.', 'base_model.')
                                elif key.startswith('base_model.model.'):
                                    clean_key = key
                                else:
                                    clean_key = f'base_model.{key}'

                                clean_weights[clean_key] = value

                            # ê°€ì¤‘ì¹˜ ë¡œë“œ (strict=Falseë¡œ ëˆ„ë½ëœ í‚¤ ë¬´ì‹œ)
                            missing_keys, unexpected_keys = self.model.load_state_dict(clean_weights, strict=False)

                            if missing_keys:
                                print(f"âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                            if unexpected_keys:
                                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")

                            print("âœ… ë°©ë²• 3 ì„±ê³µ: ìˆ˜ë™ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ë¡œë“œ")
                        else:
                            raise FileNotFoundError("adapter_model.safetensors íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                    except Exception as manual_error:
                        print(f"âŒ ëª¨ë“  PEFT ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {manual_error}")
                        raise manual_error

            # 3ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")

            try:
                # ì €ì¥ëœ í† í¬ë‚˜ì´ì € ì‹œë„
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
                print("âœ… ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")

            except Exception as tokenizer_error:
                print(f"âš ï¸ ì €ì¥ëœ í† í¬ë‚˜ì´ì € ì‹¤íŒ¨: {tokenizer_error}")

                # ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name,
                    trust_remote_code=True
                )
                print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")

            # í† í¬ë‚˜ì´ì € ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id

            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()

            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ì „ ì„±ê³µ!")
            print(f"ğŸ“Š ëª¨ë¸ íƒ€ì…: {type(self.model).__name__}")
            print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {next(self.model.parameters()).device}")
            print(f"ğŸ“ í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸°: {len(self.tokenizer)}")

            return self.model, self.tokenizer

        except Exception as e:
            print(f"âŒ ì „ì²´ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"ğŸ“‹ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")

            # êµ¬ì²´ì ì¸ í•´ê²°ì±… ì œì‹œ
            error_str = str(e)
            if "CUDA out of memory" in error_str:
                print("ğŸ’¡ í•´ê²°ì±…: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - CPU ëª¨ë“œë¡œ ì¬ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ GPU ì‚¬ìš©")
            elif "None is not a local folder" in error_str:
                print("ğŸ’¡ í•´ê²°ì±…: adapter_config.jsonì˜ base_model_name_or_path ìˆ˜ì • í•„ìš”")
            elif "KeyError" in error_str or "missing" in error_str.lower():
                print("ğŸ’¡ í•´ê²°ì±…: í‚¤ ë§¤í•‘ ë¬¸ì œ - ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ì¬ìƒì„± í•„ìš”")
            else:
                print("ğŸ’¡ í•´ê²°ì±…: ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë² ì´ìŠ¤ ëª¨ë¸ ì‹œë„")

            raise e

    def _load_cpu_mode_fixed(self, model_path, base_model_name):
        """CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ (ìˆ˜ì • ë²„ì „)"""
        print("ğŸ”„ CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")

        try:
            # CPU ì „ìš© ë¡œë”© (ê°•ì œ ë² ì´ìŠ¤ ëª¨ë¸ ì§€ì •)
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,  # ê°•ì œë¡œ ì§€ì •ëœ ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )

            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            self.model = PeftModel.from_pretrained(base_model, model_path)
            self.model.eval()

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
            except:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name,
                    trust_remote_code=True
                )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id

            print(f"âœ… CPU ëª¨ë“œ ë¡œë”© ì™„ë£Œ")
            print(f"âš ï¸ CPU ëª¨ë“œëŠ” ì¶”ë¡  ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            return self.model, self.tokenizer

        except Exception as e:
            print(f"âŒ CPU ëª¨ë“œ ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
            raise e

    def generate_answer(self, question, category="default", **kwargs):
        """ìµœì¢… ìˆ˜ì •ëœ ë‹µë³€ ìƒì„± ë©”ì„œë“œ"""

        if self.model is None or self.tokenizer is None:
            raise ValueError("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        try:
            # 1. CPU ëª¨ë“œ ê°•ì œ ì „í™˜
            if self.model.device != torch.device('cpu'):
                print("ğŸ”§ ì•ˆì „ì„ ìœ„í•´ CPU ëª¨ë“œë¡œ ì „í™˜...")
                self.model = self.model.to('cpu').float()

            # 2. ë§¤ìš° ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (íŠ¹ìˆ˜ í† í° í”¼í•¨)
            system_content = self.system_prompts.get(category, self.system_prompts["default"])

            # ì±„íŒ… í…œí”Œë¦¿ ì™„ì „ íšŒí”¼
            simple_prompt = f"ì§ˆë¬¸: {question}\në‹µë³€:"

            # 3. ì•ˆì „í•œ í† í¬ë‚˜ì´ì§• (ë² ì´ìŠ¤ í† í¬ë‚˜ì´ì €ë§Œ ì‚¬ìš©)
            try:
                inputs = self.tokenizer(
                    simple_prompt,
                    return_tensors="pt",
                    max_length=400,
                    truncation=True,
                    padding=False,  # íŒ¨ë”© ë¹„í™œì„±í™”
                    add_special_tokens=True
                )
            except Exception as tokenize_error:
                print(f"âš ï¸ í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨, ë” ë‹¨ìˆœí™”: {tokenize_error}")
                inputs = self.tokenizer(
                    f"{question}",
                    return_tensors="pt",
                    max_length=200,
                    truncation=True
                )

            # 4. í† í° ID ì•ˆì „ì„± í™•ë³´
            vocab_size = len(self.tokenizer)

            # ì…ë ¥ í† í° ID ê²€ì¦
            input_ids = inputs['input_ids']
            max_id = input_ids.max().item()
            min_id = input_ids.min().item()

            print(f"í† í° ID ë²”ìœ„: {min_id} ~ {max_id}, ì–´íœ˜ í¬ê¸°: {vocab_size}")

            # ë²”ìœ„ ì´ˆê³¼ í† í° ìˆ˜ì •
            if max_id >= vocab_size:
                print(f"âš ï¸ í† í° ID ë²”ìœ„ ì´ˆê³¼ ìˆ˜ì •: {max_id} -> {vocab_size - 1}")
                input_ids = torch.clamp(input_ids, 0, vocab_size - 1)
                inputs['input_ids'] = input_ids

            # ìŒìˆ˜ í† í° ID ìˆ˜ì •
            if min_id < 0:
                print(f"âš ï¸ ìŒìˆ˜ í† í° ID ìˆ˜ì •: {min_id} -> 0")
                input_ids = torch.clamp(input_ids, 0, vocab_size - 1)
                inputs['input_ids'] = input_ids

            # 5. ë§¤ìš° ë³´ìˆ˜ì ì¸ ìƒì„± ì„¤ì •
            generation_config = {
                "input_ids": inputs['input_ids'],
                "max_new_tokens": min(kwargs.get("max_new_tokens", 100), 100),
                "do_sample": False,  # ê·¸ë¦¬ë”” ë””ì½”ë”©ë§Œ
                "num_beams": 1,
                "early_stopping": True,
                "pad_token_id": self.tokenizer.eos_token_id,  # EOSë¥¼ íŒ¨ë”©ìœ¼ë¡œ ì‚¬ìš©
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": False,
                "output_attentions": False,
                "output_hidden_states": False,
                "return_dict_in_generate": False
            }

            # attention_maskê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if 'attention_mask' in inputs:
                generation_config["attention_mask"] = inputs['attention_mask']

            # 6. ìƒì„± ì‹¤í–‰
            start_time = time.time()

            self.model.eval()

            print("ğŸ¯ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...")

            with torch.no_grad():
                try:
                    outputs = self.model.generate(**generation_config)
                except Exception as gen_error:
                    print(f"âš ï¸ ìƒì„± ì˜¤ë¥˜ ë°œìƒ: {gen_error}")

                    # ë” ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                    simple_config = {
                        "input_ids": inputs['input_ids'],
                        "max_length": inputs['input_ids'].shape[1] + 50,
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False
                    }

                    outputs = self.model.generate(**simple_config)

            generation_time = time.time() - start_time

            # 7. ì•ˆì „í•œ ë””ì½”ë”©
            try:
                # ì…ë ¥ ê¸¸ì´ë§Œí¼ ì œê±°
                input_length = inputs['input_ids'].shape[1]
                generated_tokens = outputs[0][input_length:]

                # ìƒì„±ëœ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
                if len(generated_tokens) > 0:
                    response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                else:
                    response = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            except Exception as decode_error:
                print(f"âš ï¸ ë””ì½”ë”© ì˜¤ë¥˜: {decode_error}")
                try:
                    # ì „ì²´ ì¶œë ¥ ë””ì½”ë”© í›„ ì…ë ¥ ì œê±°
                    full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    input_text = self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
                    response = full_response.replace(input_text, "").strip()

                    if not response:
                        response = "ë‹µë³€ ë””ì½”ë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

                except:
                    response = "ë””ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            print(f"âœ… ìƒì„± ì™„ë£Œ: {len(response)}ì")

            return {
                "answer": response.strip(),
                "generation_time": generation_time,
                "tokens_generated": len(outputs[0]) - len(inputs['input_ids'][0]),
                "device": "cpu",
                "safe_mode": True,
                "vocab_size": vocab_size
            }

        except Exception as e:
            print(f"âŒ ì „ì²´ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}",
                "generation_time": 0,
                "tokens_generated": 0,
                "device": "cpu",
                "error": str(e)
            }

    def batch_generate(self, questions, categories=None, **kwargs):
        """
        ë°°ì¹˜ ì¶”ë¡ 

        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            categories: ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
        """
        if categories is None:
            categories = ["default"] * len(questions)

        if len(questions) != len(categories):
            raise ValueError("ì§ˆë¬¸ ìˆ˜ì™€ ì¹´í…Œê³ ë¦¬ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        results = []
        total_time = 0

        for i, (question, category) in enumerate(zip(questions, categories)):
            print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(questions)}")

            result = self.generate_answer(question, category, **kwargs)
            results.append({
                "question": question,
                "category": category,
                **result
            })
            total_time += result["generation_time"]

        return {
            "results": results,
            "total_time": total_time,
            "average_time": total_time / len(questions)
        }

    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ"""
        print("=== ë¯¼ë²• ì „ë¬¸ê°€ ì±„íŒ… ëª¨ë“œ ===")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì¹´í…Œê³ ë¦¬ë¥¼ ì§€ì •í•˜ë ¤ë©´ 'ì¹´í…Œê³ ë¦¬:ì§ˆë¬¸' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì˜ˆ: ì „ì„¸ì‚¬ê¸°:ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì§€ ëª»í–ˆì–´ìš”")
        print("-" * 50)

        chat_history = []

        while True:
            user_input = input("\nì§ˆë¬¸: ").strip()

            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_input:
                continue

            # ì¹´í…Œê³ ë¦¬ íŒŒì‹±
            if ':' in user_input:
                category, question = user_input.split(':', 1)
                category = category.strip()
                question = question.strip()
            else:
                category = "default"
                question = user_input

            try:
                result = self.generate_answer(question, category)

                print(f"\n[{category}] ì „ë¬¸ê°€ ë‹µë³€:")
                print(result["answer"])
                print(f"\n(ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ)")

                # ì±„íŒ… ê¸°ë¡ ì €ì¥
                chat_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "question": question,
                    "category": category,
                    "answer": result["answer"],
                    "generation_time": result["generation_time"]
                })

            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

        return chat_history

    def evaluate_on_test_cases(self, test_cases):
        """
        í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•œ í‰ê°€

        Args:
            test_cases: [{"question": str, "category": str, "expected": str (ì„ íƒ)}, ...]
        """
        print("=== í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€ ===")

        results = []

        for i, test_case in enumerate(test_cases, 1):
            question = test_case["question"]
            category = test_case.get("category", "default")
            expected = test_case.get("expected", None)

            print(f"\n{i}. [{category}] í…ŒìŠ¤íŠ¸")
            print(f"ì§ˆë¬¸: {question}")

            result = self.generate_answer(question, category)

            print(f"ìƒì„±ëœ ë‹µë³€: {result['answer']}")

            evaluation = {
                "test_id": i,
                "question": question,
                "category": category,
                "generated_answer": result["answer"],
                "generation_time": result["generation_time"],
                "tokens_generated": result["tokens_generated"]
            }

            if expected:
                evaluation["expected_answer"] = expected
                # ê°„ë‹¨í•œ ìœ ì‚¬ì„± í‰ê°€ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ ë©”íŠ¸ë¦­ ì‚¬ìš©)
                similarity = self._calculate_similarity(result["answer"], expected)
                evaluation["similarity_score"] = similarity

            results.append(evaluation)
            print("-" * 80)

        return results

    def _calculate_similarity(self, generated, expected):
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ê³„ì‚° (BLEU ìŠ¤ì½”ì–´ ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ì„±
        gen_words = set(generated.lower().split())
        exp_words = set(expected.lower().split())

        if not exp_words:
            return 0.0

        intersection = gen_words.intersection(exp_words)
        return len(intersection) / len(exp_words)

    def get_default_test_cases(self):
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë°˜í™˜"""
        return [
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ì „ì„¸ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì§€ ëª»í•  ê²ƒ ê°™ì€ë°, ì–´ë–¤ ë²•ì  ì ˆì°¨ë¥¼ ë°Ÿì„ ìˆ˜ ìˆë‚˜ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ê·¼ì €ë‹¹ê¶Œì´ ì„¤ì •ëœ ë¶€ë™ì‚°ì„ ë§¤ìˆ˜í•  ë•Œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            },
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ì „ì„¸ ê³„ì•½ ì „ì— ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  ì„œë¥˜ë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ë¶€ë™ì‚° ì·¨ë“ì‹œíš¨ê°€ ì™„ì„±ë˜ë ¤ë©´ ì–´ë–¤ ì¡°ê±´ë“¤ì´ í•„ìš”í•œê°€ìš”?",
            },
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ê¹¡í†µì „ì„¸ë¥¼ í”¼í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ì „ì„¸ê¶Œê³¼ ì„ì°¨ê¶Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            }
        ]

    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=== ë¯¼ë²• ì „ë¬¸ê°€ ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸ ===")

        test_cases = self.get_default_test_cases()
        results = self.evaluate_on_test_cases(test_cases)

        # í†µê³„ ê³„ì‚°
        total_time = sum(r["generation_time"] for r in results)
        avg_time = total_time / len(results)
        total_tokens = sum(r["tokens_generated"] for r in results)
        avg_tokens = total_tokens / len(results)

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for result in results:
            category = result["category"]
            if category not in category_stats:
                category_stats[category] = {"count": 0, "total_time": 0, "total_tokens": 0}

            category_stats[category]["count"] += 1
            category_stats[category]["total_time"] += result["generation_time"]
            category_stats[category]["total_tokens"] += result["tokens_generated"]

        # ê²°ê³¼ ìš”ì•½
        summary = {
            "test_results": results,
            "overall_stats": {
                "total_tests": len(results),
                "total_time": total_time,
                "average_time": avg_time,
                "total_tokens": total_tokens,
                "average_tokens": avg_tokens,
                "tokens_per_second": total_tokens / total_time if total_time > 0 else 0
            },
            "category_stats": {
                cat: {
                    "count": stats["count"],
                    "avg_time": stats["total_time"] / stats["count"],
                    "avg_tokens": stats["total_tokens"] / stats["count"]
                }
                for cat, stats in category_stats.items()
            },
            "timestamp": datetime.now().isoformat()
        }

        print(f"\n=== í…ŒìŠ¤íŠ¸ ìš”ì•½ ===")
        print(f"ì´ í…ŒìŠ¤íŠ¸: {summary['overall_stats']['total_tests']}ê°œ")
        print(f"í‰ê·  ìƒì„± ì‹œê°„: {summary['overall_stats']['average_time']:.2f}ì´ˆ")
        print(f"í‰ê·  í† í° ìˆ˜: {summary['overall_stats']['average_tokens']:.1f}ê°œ")
        print(f"ìƒì„± ì†ë„: {summary['overall_stats']['tokens_per_second']:.1f} í† í°/ì´ˆ")

        return summary

    def save_test_results(self, results, output_path):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

    def benchmark_generation_speed(self, num_tests=10):
        """ìƒì„± ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        print(f"=== ìƒì„± ì†ë„ ë²¤ì¹˜ë§ˆí¬ ({num_tests}íšŒ í…ŒìŠ¤íŠ¸) ===")

        test_question = "ì „ì„¸ ê³„ì•½ì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        times = []
        token_counts = []

        for i in range(num_tests):
            print(f"í…ŒìŠ¤íŠ¸ {i+1}/{num_tests}")
            result = self.generate_answer(test_question, "ì „ì„¸ì‚¬ê¸°")
            times.append(result["generation_time"])
            token_counts.append(result["tokens_generated"])

        avg_time = sum(times) / len(times)
        avg_tokens = sum(token_counts) / len(token_counts)
        avg_speed = avg_tokens / avg_time if avg_time > 0 else 0

        benchmark_result = {
            "num_tests": num_tests,
            "times": times,
            "token_counts": token_counts,
            "average_time": avg_time,
            "average_tokens": avg_tokens,
            "average_speed_tokens_per_sec": avg_speed,
            "min_time": min(times),
            "max_time": max(times)
        }

        print(f"í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ")
        print(f"í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
        print(f"í‰ê·  ì†ë„: {avg_speed:.1f} í† í°/ì´ˆ")
        print(f"ìµœì†Œ/ìµœëŒ€ ì‹œê°„: {min(times):.2f}ì´ˆ / {max(times):.2f}ì´ˆ")

        return benchmark_result

    def update_generation_config(self, **kwargs):
        """ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.generation_config.update(kwargs)
        print(f"ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸: {kwargs}")

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model is None:
            return None

        info = {
            "model_type": type(self.model).__name__,
            "device": str(self.model.device) if hasattr(self.model, 'device') else "Unknown",
            "dtype": str(self.model.dtype) if hasattr(self.model, 'dtype') else "Unknown",
        }

        if hasattr(self.model, 'peft_config'):
            info["peft_type"] = "LoRA"
            info["trainable_params"] = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        return info

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    engine = InferenceEngine()

    # Fine-tuned ëª¨ë¸ ë¡œë“œ
    model_path = "./llama-1b-civil-law-specialist"
    engine.load_finetuned_model(model_path)

    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = engine.run_comprehensive_test()

    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
    engine.save_test_results(test_results, "test_results.json")

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_results = engine.benchmark_generation_speed()

    # ëŒ€í™”í˜• ëª¨ë“œ (ì„ íƒì‚¬í•­)
    # chat_history = engine.interactive_chat()

    print("\n=== ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
    print("ì‚¬ìš© ë°©ë²•:")
    print("1. engine.generate_answer('ì§ˆë¬¸', 'ì¹´í…Œê³ ë¦¬')")
    print("2. engine.interactive_chat()")
    print("3. engine.run_comprehensive_test()")