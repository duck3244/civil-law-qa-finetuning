#!/usr/bin/env python3
# test_model.py
# ë…ë¦½ì ì¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )

import torch
import os
import gc
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def clear_memory():
    """ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def test_model_cpu(model_path, base_model_name="meta-llama/Llama-3.2-1B"):
    """CPU ëª¨ë“œë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”„ CPU ëª¨ë“œë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # CPU ì „ìš© ì„¤ì •
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        print("ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë”©...")
        model = PeftModel.from_pretrained(base_model, model_path)
        model.eval()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_cases = [
            {
                "category": "ì „ì„¸ì‚¬ê¸°",
                "question": "ì „ì„¸ë³´ì¦ê¸ˆì„ ì•ˆì „í•˜ê²Œ ë³´í˜¸í•˜ëŠ” ë°©ë²•ì€?"
            },
            {
                "category": "ë¶€ë™ì‚°ë¬¼ê¶Œ",
                "question": "ê·¼ì €ë‹¹ê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
            },
            {
                "category": "ì „ì„¸ì‚¬ê¸°", 
                "question": "ê¹¡í†µì „ì„¸ë¥¼ í”¼í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
            }
        ]
        
        print("\n=== ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}: [{test_case['category']}]")
            print(f"â“ ì§ˆë¬¸: {test_case['question']}")
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if test_case['category'] == "ì „ì„¸ì‚¬ê¸°":
                system_content = "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ì „ì„¸ì‚¬ê¸° ë° ì„ëŒ€ì°¨ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
            else:
                system_content = "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶€ë™ì‚°ë¬¼ê¶Œ ë¶„ì•¼ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": test_case['question']}
            ]
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            prompt = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = tokenizer(prompt, return_tensors="pt")
            
            # ë‹µë³€ ìƒì„±
            print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
            import time
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True
                )
            
            generation_time = time.time() - start_time
            
            # ê²°ê³¼ ì¶œë ¥
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            )
            
            print(f"ğŸ’¬ ë‹µë³€: {response.strip()}")
            print(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            print("-" * 80)
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ CPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if 'model' in locals():
            del model
        if 'base_model' in locals():
            del base_model
        if 'tokenizer' in locals():
            del tokenizer
        clear_memory()

def test_model_gpu(model_path, base_model_name="meta-llama/Llama-3.2-1B"):
    """GPU ëª¨ë“œë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    print("ğŸ”„ GPU ëª¨ë“œë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # CUDA í™˜ê²½ ë³µêµ¬
    if "CUDA_VISIBLE_DEVICES" in os.environ:
        del os.environ["CUDA_VISIBLE_DEVICES"]
    
    clear_memory()
    
    try:
        from transformers import BitsAndBytesConfig
        
        # 4bit ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=False,
        )
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (4bit)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            max_memory={0: "4GB", "cpu": "16GB"}
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        print("ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë”©...")
        model = PeftModel.from_pretrained(base_model, model_path)
        model.eval()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… GPU ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        question = "ì „ì„¸ ê³„ì•½ ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ì€?"
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": question}
        ]
        
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        print(f"â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {question}")
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        
        import time
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        response = tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        print(f"ğŸ’¬ ë‹µë³€: {response.strip()}")
        print(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
        print("âœ… GPU í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        return True
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
        print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
        return False
        
    except Exception as e:
        print(f"âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    finally:
        if 'model' in locals():
            del model
        if 'base_model' in locals():
            del base_model
        if 'tokenizer' in locals():
            del tokenizer
        clear_memory()

def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--model_path", type=str, default="./llama-1b-civil-law-specialist",
                       help="Fine-tuned ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--base_model", type=str, default="meta-llama/Llama-3.2-1B",
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--mode", type=str, choices=["cpu", "gpu", "auto"], default="auto",
                       help="ì‹¤í–‰ ëª¨ë“œ")
    
    args = parser.parse_args()
    
    print("ğŸ§ª ë…ë¦½ì ì¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {args.model_path}")
    print(f"ğŸ¤– ë² ì´ìŠ¤ ëª¨ë¸: {args.base_model}")
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not os.path.exists(args.model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model_path}")
        return
    
    success = False
    
    if args.mode == "cpu":
        success = test_model_cpu(args.model_path, args.base_model)
    elif args.mode == "gpu":
        success = test_model_gpu(args.model_path, args.base_model)
    else:  # auto
        # GPU ë¨¼ì € ì‹œë„, ì‹¤íŒ¨í•˜ë©´ CPU
        if torch.cuda.is_available():
            print("ğŸ”„ GPU ëª¨ë“œë¶€í„° ì‹œë„...")
            success = test_model_gpu(args.model_path, args.base_model)
            
        if not success:
            print("ğŸ”„ CPU ëª¨ë“œë¡œ ì¬ì‹œë„...")
            success = test_model_cpu(args.model_path, args.base_model)
    
    if success:
        print("\nğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("ğŸ’¡ ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
        print("   python main.py --mode interactive --model_path", args.model_path)
    else:
        print("\nâŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
