# main.py
# ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

import argparse
import os
import sys
import torch
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸
from core.data_loader import CivilLawDataLoader
from core.data_formatter import DataFormatter
from core.model_setup import ModelManager
from core.trainer_config import TrainerManager
from core.inference_engine import InferenceEngine

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¯¼ë²• QA ë°ì´í„°ì…‹ Fine-tuning")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_file", type=str, default="civil_law_qa_dataset.csv",
                       help="ë°ì´í„°ì…‹ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./llama-1b-civil-law-specialist",
                       help="ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B",
                       help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")
    
    # ëª¨ë“œ ì„ íƒ
    parser.add_argument("--mode", type=str, choices=["train", "inference", "test", "interactive"],
                       default="train", help="ì‹¤í–‰ ëª¨ë“œ")
    
    # í›ˆë ¨ ì„¤ì •
    parser.add_argument("--preset", type=str, choices=["fast", "balanced", "quality", "memory_efficient"],
                       default="balanced", help="í›ˆë ¨ í”„ë¦¬ì…‹")
    parser.add_argument("--epochs", type=int, default=None, help="í›ˆë ¨ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=None, help="í•™ìŠµë¥ ")
    parser.add_argument("--batch_size", type=int, default=None, help="ë°°ì¹˜ í¬ê¸°")
    
    # ë°ì´í„° í•„í„°ë§
    parser.add_argument("--categories", type=str, nargs="+", default=None,
                       help="ì‚¬ìš©í•  ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ì„¸ì‚¬ê¸° ë¶€ë™ì‚°ë¬¼ê¶Œ)")
    parser.add_argument("--difficulties", type=str, nargs="+", default=None,
                       help="ì‚¬ìš©í•  ë‚œì´ë„ (ì˜ˆ: ì´ˆê¸‰ ì¤‘ê¸‰ ê³ ê¸‰)")
    
    # ì¶”ë¡  ì„¤ì •
    parser.add_argument("--model_path", type=str, default=None,
                       help="ì¶”ë¡ ìš© ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--test_file", type=str, default=None,
                       help="í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ íŒŒì¼")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ›ï¸  ë¯¼ë²• QA ë°ì´í„°ì…‹ Fine-tuning ì‹œìŠ¤í…œ")
    print("=" * 60)
    print(f"ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    if args.mode == "train":
        run_training(args)
    elif args.mode == "inference":
        run_inference(args)
    elif args.mode == "test":
        run_testing(args)
    elif args.mode == "interactive":
        run_interactive(args)

def run_training(args):
    """í›ˆë ¨ ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸš€ í›ˆë ¨ ëª¨ë“œ ì‹œì‘")
    
    # 1. ë°ì´í„° ë¡œë”©
    print("\nğŸ“ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©")
    loader = CivilLawDataLoader(args.data_file)
    df = loader.load_dataset()
    
    if df is None:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        sys.exit(1)
    
    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    print("\nğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")
    df = loader.preprocess_data()
    
    # ì¹´í…Œê³ ë¦¬/ë‚œì´ë„ í•„í„°ë§
    if args.categories:
        df = loader.filter_by_category(args.categories)
    
    if args.difficulties:
        df = loader.filter_by_difficulty(args.difficulties)
    
    if len(df) == 0:
        print("âŒ í•„í„°ë§ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)
    
    # 3. ë°ì´í„° í˜•ì‹ ë³€í™˜
    print("\nğŸ“ 3ë‹¨ê³„: ë°ì´í„° í˜•ì‹ ë³€í™˜")
    formatter = DataFormatter()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê· ë“± ë¶„í• 
    train_df, eval_df = formatter.stratified_split_by_category(df)
    
    # Conversational í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    train_dataset = formatter.to_conversational_format(train_df)
    eval_dataset = formatter.to_conversational_format(eval_df)
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}ê°œ")
    
    # 4. ëª¨ë¸ ì„¤ì •
    print("\nğŸ¤– 4ë‹¨ê³„: ëª¨ë¸ ì„¤ì •")
    model_manager = ModelManager(args.model_name)
    model, tokenizer = model_manager.load_model_and_tokenizer()
    
    # LoRA ì„¤ì • ë° ì ìš©
    peft_config = model_manager.setup_lora_config()
    model = model_manager.apply_lora()
    
    # 5. í›ˆë ¨ ì„¤ì •
    print("\nâš™ï¸ 5ë‹¨ê³„: í›ˆë ¨ ì„¤ì •")
    trainer_manager = TrainerManager(args.output_dir)
    
    # í”„ë¦¬ì…‹ ê¸°ë°˜ ì„¤ì • ìƒì„±
    custom_config = {}
    if args.epochs:
        custom_config["num_train_epochs"] = args.epochs
    if args.learning_rate:
        custom_config["learning_rate"] = args.learning_rate
    if args.batch_size:
        custom_config["per_device_train_batch_size"] = args.batch_size
    
    training_args = trainer_manager.create_training_config_from_preset(
        preset=args.preset,
        **custom_config
    )
    
    # í›ˆë ¨ ì‹œê°„ ì¶”ì •
    time_estimate = trainer_manager.estimate_training_time(
        len(train_dataset), args.preset
    )
    print(f"â±ï¸ ì˜ˆìƒ í›ˆë ¨ ì‹œê°„: {time_estimate['estimated_times']['default']}")
    
    # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    print("\nğŸ¯ 6ë‹¨ê³„: í›ˆë ¨ ì‹œì‘")
    trainer = trainer_manager.create_trainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        peft_config=peft_config,
        tokenizer=tokenizer
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    train_result = trainer_manager.train()
    
    # 7. ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ 7ë‹¨ê³„: ëª¨ë¸ ì €ì¥")
    model_path = trainer_manager.save_model()
    
    # í›ˆë ¨ ì •ë³´ ì €ì¥
    trainer_manager.save_training_info(
        dataset_stats=loader.get_stats(),
        model_info=model_manager.model_info
    )
    
    model_manager.save_model_config(args.output_dir)
    
    print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“‚ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
    
    # 8. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì •ë¦¬ ê°œì„ )
    print("\nğŸ§ª 8ë‹¨ê³„: í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸")
    try:
        # ì² ì €í•œ ë©”ëª¨ë¦¬ ì •ë¦¬
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ê³¼ íŠ¸ë ˆì´ë„ˆ ì™„ì „ ì œê±°
        if 'model' in locals():
            del model
        if 'trainer' in locals():
            del trainer
        if 'trainer_manager' in locals():
            del trainer_manager
        if 'model_manager' in locals():
            del model_manager
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ í˜„í™© ì¶œë ¥
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: í• ë‹¹ {allocated:.2f}GB, ì˜ˆì•½ {reserved:.2f}GB")
        
        # ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ ëŒ€ê¸°)
        import time
        time.sleep(2)
        
        # ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ê¶Œì¥)
        print("ğŸ’¡ ì¶”ë¡  í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:")
        print(f"   python main.py --mode test --model_path {model_path}")
        print(f"   python main.py --mode interactive --model_path {model_path}")
        
        # ê°„ë‹¨í•œ CPU í…ŒìŠ¤íŠ¸ ì‹œë„
        print("\nğŸ”¬ CPU ëª¨ë“œë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹œë„...")
        try:
            # CPU ì „ìš©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
            
            engine = InferenceEngine()
            
            # CPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from peft import PeftModel
            
            base_model = AutoModelForCausalLM.from_pretrained(
                args.model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            cpu_model = PeftModel.from_pretrained(base_model, model_path)
            cpu_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            engine.model = cpu_model
            engine.tokenizer = cpu_tokenizer
            
            if engine.tokenizer.pad_token is None:
                engine.tokenizer.pad_token = engine.tokenizer.eos_token
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_question = "ì „ì„¸ ê³„ì•½ ì‹œ ì£¼ì˜ì‚¬í•­ì€?"
            result = engine.generate_answer(test_question, "ì „ì„¸ì‚¬ê¸°", max_new_tokens=50)
            
            print(f"âœ… CPU í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_question}")
            print(f"ğŸ¤– ëª¨ë¸ ë‹µë³€: {result['answer'][:100]}...")
            print(f"â±ï¸ ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
            
            # CPU ëª¨ë“œ ì •ë¦¬
            del cpu_model, cpu_tokenizer, base_model, engine
            gc.collect()
            
            # CUDA í™˜ê²½ ë³µêµ¬
            if "CUDA_VISIBLE_DEVICES" in os.environ:
                del os.environ["CUDA_VISIBLE_DEVICES"]
            
        except Exception as cpu_error:
            print(f"âš ï¸ CPU í…ŒìŠ¤íŠ¸ë„ ì‹¤íŒ¨: {cpu_error}")
            print("ëª¨ë¸ íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ì˜¤ë¥˜: {e}")
        print("âœ… í›ˆë ¨ì€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‚ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
        print("\nğŸ’¡ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:")
        print("   1. í„°ë¯¸ë„ì„ ìƒˆë¡œ ì—´ê³ ")
        print(f"   2. python main.py --mode test --model_path {model_path}")
        print("   3. ë˜ëŠ” python main.py --mode interactive")

def run_inference(args):
    """ì¶”ë¡  ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ”® ì¶”ë¡  ëª¨ë“œ ì‹œì‘")
    
    model_path = args.model_path or args.output_dir
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)
    
    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)
    
    # ëŒ€í™”í˜• ì¶”ë¡ 
    print("\nğŸ’¬ ëŒ€í™”í˜• ì¶”ë¡  ì‹œì‘")
    chat_history = engine.interactive_chat()
    
    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    chat_file = f"chat_history_{timestamp}.json"
    engine.save_test_results(chat_history, chat_file)
    print(f"ì±„íŒ… ê¸°ë¡ ì €ì¥: {chat_file}")

def run_testing(args):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘")
    
    model_path = args.model_path or args.output_dir
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)
    
    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\nğŸ“Š ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    test_results = engine.run_comprehensive_test()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print("\nâš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    benchmark_results = engine.benchmark_generation_speed()
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    test_file = f"test_results_{timestamp}.json"
    benchmark_file = f"benchmark_results_{timestamp}.json"
    
    engine.save_test_results(test_results, test_file)
    engine.save_test_results(benchmark_results, benchmark_file)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_file}")
    print(f"âš¡ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: {benchmark_file}")

def run_interactive(args):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘")
    
    model_path = args.model_path or args.output_dir
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ë¨¼ì € í›ˆë ¨ì„ ì™„ë£Œí•˜ê±°ë‚˜ ì˜¬ë°”ë¥¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    
    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)
    
    # ëŒ€í™”í˜• ì±„íŒ… ì‹œì‘
    chat_history = engine.interactive_chat()
    
    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    if chat_history:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chat_file = f"chat_history_{timestamp}.json"
        engine.save_test_results(chat_history, chat_file)
        print(f"\nì±„íŒ… ê¸°ë¡ ì €ì¥: {chat_file}")

def print_usage_examples():
    """ì‚¬ìš© ì˜ˆì‹œ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“š ì‚¬ìš© ì˜ˆì‹œ")
    print("="*60)
    print()
    print("1. ê¸°ë³¸ í›ˆë ¨:")
    print("   python main.py --mode train")
    print()
    print("2. ê³ í’ˆì§ˆ í›ˆë ¨ (ë” ë§ì€ ì—í¬í¬):")
    print("   python main.py --mode train --preset quality --epochs 6")
    print()
    print("3. íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í›ˆë ¨:")
    print("   python main.py --mode train --categories ì „ì„¸ì‚¬ê¸°")
    print()
    print("4. ì¶”ë¡  ëª¨ë“œ:")
    print("   python main.py --mode inference")
    print()
    print("5. í…ŒìŠ¤íŠ¸ ëª¨ë“œ:")
    print("   python main.py --mode test")
    print()
    print("6. ëŒ€í™”í˜• ëª¨ë“œ:")
    print("   python main.py --mode interactive")
    print()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\n" + "="*60)
        print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("="*60)
