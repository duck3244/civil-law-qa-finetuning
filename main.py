# improved_main.py
# ëª¨ë“  ë¬¸ì œì ì´ í•´ê²°ëœ ê°œì„ ëœ í›ˆë ¨ ì‹œìŠ¤í…œ

import argparse
import os
import sys
import torch
import json
import shutil
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸
from core.data_loader import CivilLawDataLoader
from core.data_formatter import DataFormatter
from core.model_setup import ModelManager
from core.trainer_config import TrainerManager
from core.inference_engine import InferenceEngine


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¯¼ë²• QA ë°ì´í„°ì…‹ Fine-tuning (ê°œì„ ëœ ë²„ì „)")

    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_file", type=str, default="civil_law_qa_dataset.csv",
                        help="ë°ì´í„°ì…‹ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./llama-1b-civil-law-specialist-v2",
                        help="ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B",
                        help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„")

    # ëª¨ë“œ ì„ íƒ
    parser.add_argument("--mode", type=str, choices=["train", "inference", "test", "interactive"],
                        default="train", help="ì‹¤í–‰ ëª¨ë“œ")

    # í›ˆë ¨ ì„¤ì • (ê°œì„ ëœ ê¸°ë³¸ê°’)
    parser.add_argument("--preset", type=str,
                        choices=["fast", "balanced", "quality", "memory_efficient", "ultra_light"],
                        default="quality", help="í›ˆë ¨ í”„ë¦¬ì…‹")
    parser.add_argument("--epochs", type=int, default=8, help="í›ˆë ¨ ì—í¬í¬ ìˆ˜ (ì¦ê°€)")
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="í•™ìŠµë¥  (ê°ì†Œ)")
    parser.add_argument("--batch_size", type=int, default=1, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--gradient_accumulation", type=int, default=16, help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…")

    # ë°ì´í„° í•„í„°ë§
    parser.add_argument("--categories", type=str, nargs="+", default=None,
                        help="ì‚¬ìš©í•  ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ì„¸ì‚¬ê¸° ë¶€ë™ì‚°ë¬¼ê¶Œ)")
    parser.add_argument("--difficulties", type=str, nargs="+", default=None,
                        help="ì‚¬ìš©í•  ë‚œì´ë„ (ì˜ˆ: ì´ˆê¸‰ ì¤‘ê¸‰ ê³ ê¸‰)")

    # ì¶”ë¡  ì„¤ì •
    parser.add_argument("--model_path", type=str, default=None,
                        help="ì¶”ë¡ ìš© ëª¨ë¸ ê²½ë¡œ")

    # ìƒˆë¡œìš´ ì˜µì…˜ë“¤
    parser.add_argument("--use_flash_attention", action="store_true",
                        help="Flash Attention ì‚¬ìš©")
    parser.add_argument("--max_length", type=int, default=1024,
                        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--save_steps", type=int, default=50,
                        help="ëª¨ë¸ ì €ì¥ ì£¼ê¸°")
    parser.add_argument("--eval_steps", type=int, default=25,
                        help="í‰ê°€ ì£¼ê¸°")
    parser.add_argument("--warmup_steps", type=int, default=100,
                        help="ì›Œë°ì—… ìŠ¤í…")

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ›ï¸  ë¯¼ë²• QA ë°ì´í„°ì…‹ Fine-tuning ì‹œìŠ¤í…œ v2.0")
    print("=" * 60)
    print(f"ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    if args.mode == "train":
        run_improved_training(args)
    elif args.mode == "inference":
        run_inference(args)
    elif args.mode == "test":
        run_testing(args)
    elif args.mode == "interactive":
        run_interactive(args)


def run_improved_training(args):
    """ê°œì„ ëœ í›ˆë ¨ ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸš€ ê°œì„ ëœ í›ˆë ¨ ëª¨ë“œ ì‹œì‘")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if os.path.exists(args.output_dir):
        backup_dir = f"{args.output_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        shutil.move(args.output_dir, backup_dir)
        print(f"ğŸ“¦ ê¸°ì¡´ ëª¨ë¸ ë°±ì—…: {backup_dir}")

    os.makedirs(args.output_dir, exist_ok=True)

    # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    print("\nğŸ“ 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬")
    loader = CivilLawDataLoader(args.data_file)
    df = loader.load_dataset()

    if df is None:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        sys.exit(1)

    # ë°ì´í„° ì „ì²˜ë¦¬
    df = loader.preprocess_data()

    # ì¹´í…Œê³ ë¦¬/ë‚œì´ë„ í•„í„°ë§
    if args.categories:
        df = loader.filter_by_category(args.categories)

    if args.difficulties:
        df = loader.filter_by_difficulty(args.difficulties)

    if len(df) == 0:
        print("âŒ í•„í„°ë§ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)

    print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„°: {len(df)}ê°œ")

    # 2. ê°œì„ ëœ ë°ì´í„° í˜•ì‹ ë³€í™˜
    print("\nğŸ“ 2ë‹¨ê³„: ê°œì„ ëœ ë°ì´í„° í˜•ì‹ ë³€í™˜")
    formatter = DataFormatter()  # ì´ì œ ImprovedDataFormatterë¥¼ ê°€ë¦¬í‚´

    # ë°ì´í„° ì¦ê°• (í’ˆì§ˆ í–¥ìƒ)
    df = formatter.augment_data(df, augmentation_factor=1.3)
    print(f"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(df)}ê°œ")

    # ì¹´í…Œê³ ë¦¬ë³„ ê· ë“± ë¶„í•  (ê°œì„ ëœ ë©”ì„œë“œ)
    train_df, eval_df = formatter.split_train_eval_stratified(df, test_size=0.2)

    # ê°œì„ ëœ conversational í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë” ë‚˜ì€ í”„ë¡¬í”„íŠ¸)
    train_dataset = formatter.to_conversational_format(
        train_df,
        include_category=True,
        include_difficulty=True,
        enhanced_prompts=True  # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    )
    eval_dataset = formatter.to_conversational_format(
        eval_df,
        include_category=True,
        include_difficulty=True,
        enhanced_prompts=True
    )

    # ê¸¸ì´ ì»¬ëŸ¼ ì¶”ê°€ (group_by_lengthìš©)
    train_dataset = formatter.add_length_column(train_dataset)
    eval_dataset = formatter.add_length_column(eval_dataset)

    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}ê°œ")

    # 3. ê°œì„ ëœ ëª¨ë¸ ì„¤ì •
    print("\nğŸ¤– 3ë‹¨ê³„: ê°œì„ ëœ ëª¨ë¸ ì„¤ì •")
    model_manager = ModelManager(args.model_name)

    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
    model, tokenizer = model_manager.load_model_and_tokenizer(
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        use_cache=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
        attn_implementation="flash_attention_2" if args.use_flash_attention else "eager"
    )

    # ê°œì„ ëœ LoRA ì„¤ì •
    peft_config = model_manager.setup_lora_config(
        r=32,  # ì¦ê°€ëœ rank
        lora_alpha=64,  # ì¦ê°€ëœ alpha
        lora_dropout=0.1,  # ì¦ê°€ëœ dropout
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        modules_to_save=None,  # ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì„¤ì •
        base_model_name_or_path=args.model_name  # ëª…ì‹œì  ì§€ì •
    )

    model = model_manager.apply_lora()

    # 4. ê°œì„ ëœ í›ˆë ¨ ì„¤ì •
    print("\nâš™ï¸ 4ë‹¨ê³„: ê°œì„ ëœ í›ˆë ¨ ì„¤ì •")
    trainer_manager = TrainerManager(args.output_dir)

    # ì•ˆì „í•œ í›ˆë ¨ ì„¤ì • (ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì œê±°)
    safe_training_config = {
        # ê¸°ë³¸ í›ˆë ¨ ì„¤ì •
        "num_train_epochs": args.epochs,
        "learning_rate": args.learning_rate,
        "per_device_train_batch_size": args.batch_size,
        "per_device_eval_batch_size": args.batch_size,
        "gradient_accumulation_steps": args.gradient_accumulation,

        # ì‹œí€€ìŠ¤ ë° ë©”ëª¨ë¦¬ ì„¤ì •
        "max_length": args.max_length,
        "gradient_checkpointing": True,
        "bf16": True,
        "dataloader_pin_memory": False,

        # í‰ê°€ ë° ì €ì¥ ì„¤ì •
        "eval_strategy": "steps",
        "eval_steps": args.eval_steps,
        "save_strategy": "steps",
        "save_steps": args.save_steps,
        "save_total_limit": 3,
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": False,

        # ìµœì í™” ì„¤ì •
        "optim": "adamw_torch",
        "weight_decay": 0.01,
        "warmup_steps": args.warmup_steps,
        "max_grad_norm": 1.0,

        # ë¡œê¹… ì„¤ì •
        "logging_steps": 10,
        "logging_first_step": True,
        "report_to": "none",

        # ì•ˆì •ì„± ì„¤ì •
        "remove_unused_columns": False,
        "use_cache": False,
        "packing": False,  # íŒ¨í‚¹ ë¹„í™œì„±í™” (ì•ˆì •ì„±)
        "group_by_length": False,  # length ì»¬ëŸ¼ ê´€ë ¨ ë¬¸ì œ ë°©ì§€
    }

    # í”„ë¦¬ì…‹ë³„ ì¶”ê°€ ì„¤ì •
    if args.preset == "memory_efficient":
        safe_training_config.update({
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 16,
            "dataloader_pin_memory": False,
            "save_steps": 100,
            "eval_steps": 50,
        })
    elif args.preset == "quality":
        safe_training_config.update({
            "lr_scheduler_type": "cosine",
            "warmup_steps": 100,
            "save_steps": 50,
            "eval_steps": 25,
        })

    print(f"ğŸ“Š í›ˆë ¨ ì„¤ì •:")
    print(f"  ì—í¬í¬: {safe_training_config['num_train_epochs']}")
    print(f"  í•™ìŠµë¥ : {safe_training_config['learning_rate']}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {safe_training_config['per_device_train_batch_size']}")
    print(f"  ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {safe_training_config['gradient_accumulation_steps']}")
    print(f"  ìµœëŒ€ ê¸¸ì´: {safe_training_config['max_length']}")

    # í›ˆë ¨ ì„¤ì • ìƒì„±
    try:
        training_args = trainer_manager.create_training_config(**safe_training_config)
        print("âœ… í›ˆë ¨ ì„¤ì • ìƒì„± ì„±ê³µ")
    except Exception as config_error:
        print(f"âš ï¸ ê³ ê¸‰ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {config_error}")

        # ìµœì†Œ ê¸°ë³¸ ì„¤ì •
        minimal_config = {
            "num_train_epochs": args.epochs,
            "learning_rate": args.learning_rate,
            "per_device_train_batch_size": args.batch_size,
            "gradient_accumulation_steps": args.gradient_accumulation,
            "max_length": args.max_length,
            "eval_strategy": "steps",
            "eval_steps": args.eval_steps,
            "save_steps": args.save_steps,
            "logging_steps": 10,
            "report_to": "none",
            "remove_unused_columns": False,
        }

        training_args = trainer_manager.create_training_config(**minimal_config)
        print("âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í›ˆë ¨ ì„¤ì • ìƒì„± ì„±ê³µ")

    # í›ˆë ¨ ì‹œê°„ ì¶”ì •
    time_estimate = trainer_manager.estimate_training_time(
        len(train_dataset), args.preset
    )
    print(f"â±ï¸ ì˜ˆìƒ í›ˆë ¨ ì‹œê°„: {time_estimate['estimated_times']['default']}")

    # 5. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    print("\nğŸ¯ 5ë‹¨ê³„: í›ˆë ¨ ì‹œì‘")

    # ê°œì„ ëœ í¬ë§·íŒ… í•¨ìˆ˜
    def improved_formatting_func(example):
        """ê°œì„ ëœ í¬ë§·íŒ… í•¨ìˆ˜"""
        return formatter.create_enhanced_formatting_func("legal_expert")(example)

    trainer = trainer_manager.create_trainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        peft_config=peft_config,
        tokenizer=tokenizer,
        formatting_func=improved_formatting_func
    )

    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ”¥ í›ˆë ¨ ì‹œì‘...")
    train_result = trainer_manager.train()

    # 6. ê°œì„ ëœ ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ 6ë‹¨ê³„: ê°œì„ ëœ ëª¨ë¸ ì €ì¥")

    # ëª¨ë¸ ì €ì¥
    model_path = trainer_manager.save_model()

    # ì„¤ì • íŒŒì¼ ìˆ˜ì • (ë¬¸ì œ ë°©ì§€)
    fix_saved_config(args.output_dir, args.model_name)

    # í›ˆë ¨ ì •ë³´ ì €ì¥
    trainer_manager.save_training_info(
        dataset_stats=loader.get_stats(),
        model_info=model_manager.model_info
    )

    model_manager.save_model_config(args.output_dir)

    print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“‚ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")

    # 7. ì¦‰ì‹œ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª 7ë‹¨ê³„: ì¦‰ì‹œ í…ŒìŠ¤íŠ¸")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory()

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    try:
        print("ğŸ”¬ ë¹ ë¥¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        quick_quality_test(args.output_dir, args.model_name)

    except Exception as e:
        print(f"âš ï¸ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:")
        print(f"   python improved_main.py --mode test --model_path {args.output_dir}")


def fix_saved_config(output_dir, base_model_name):
    """ì €ì¥ëœ ì„¤ì • íŒŒì¼ ìˆ˜ì • (ë¬¸ì œ ë°©ì§€)"""

    print("ğŸ”§ ì„¤ì • íŒŒì¼ ìˆ˜ì • ì¤‘...")

    config_path = os.path.join(output_dir, "adapter_config.json")

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)

            # í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­
            config['base_model_name_or_path'] = base_model_name

            # modules_to_save ì œê±° (ë¬¸ì œ ë°©ì§€)
            if 'modules_to_save' in config:
                config['modules_to_save'] = None

            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)

            print("âœ… ì„¤ì • íŒŒì¼ ìˆ˜ì • ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")


def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""

    import gc

    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    gc.collect()

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def quick_quality_test(model_path, base_model_name):
    """ë¹ ë¥¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""

    print("ğŸ¯ ë¹ ë¥¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")

    # CPU ëª¨ë“œë¡œ ì•ˆì „í•˜ê²Œ í…ŒìŠ¤íŠ¸
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel

        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True
        )

        # Fine-tuned ëª¨ë¸ ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, model_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model.eval()

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "ì „ì„¸ ê³„ì•½ì—ì„œ ì£¼ì˜í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì„ëŒ€ì°¨ë³´í˜¸ë²•ì˜ ì£¼ìš” ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë³´ì¦ê¸ˆì„ ì•ˆì „í•˜ê²Œ ë³´í˜¸í•˜ëŠ” ë°©ë²•ì€?"
        ]

        print("ğŸ“ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")

        for i, question in enumerate(test_questions, 1):
            try:
                prompt = f"ì§ˆë¬¸: {question}\në‹µë³€:"
                inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=400, truncation=True)

                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_new_tokens=150,
                        do_sample=True,
                        temperature=0.3,
                        top_p=0.9,
                        repetition_penalty=1.1,
                        pad_token_id=tokenizer.eos_token_id,
                        use_cache=False
                    )

                full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = full_text.replace(prompt, "").strip()

                print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
                print(f"Q: {question}")
                print(f"A: {answer[:200]}...")

            except Exception as e:
                print(f"í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}")

        print("\nâœ… í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

        # CUDA í™˜ê²½ ë³µêµ¬
        if "CUDA_VISIBLE_DEVICES" in os.environ:
            del os.environ["CUDA_VISIBLE_DEVICES"]

    except Exception as e:
        print(f"âŒ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


def run_inference(args):
    """ì¶”ë¡  ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ”® ì¶”ë¡  ëª¨ë“œ ì‹œì‘")

    model_path = args.model_path or args.output_dir

    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)

    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)

    # ëŒ€í™”í˜• ì¶”ë¡ 
    print("\nğŸ’¬ ëŒ€í™”í˜• ì¶”ë¡  ì‹œì‘")
    chat_history = engine.interactive_chat()

    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    chat_file = f"chat_history_{timestamp}.json"
    engine.save_test_results(chat_history, chat_file)
    print(f"ì±„íŒ… ê¸°ë¡ ì €ì¥: {chat_file}")


def run_testing(args):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘")

    model_path = args.model_path or args.output_dir

    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)

    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)

    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\nğŸ“Š ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    test_results = engine.run_comprehensive_test()

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print("\nâš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    benchmark_results = engine.benchmark_generation_speed()

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    test_file = f"test_results_{timestamp}.json"
    benchmark_file = f"benchmark_results_{timestamp}.json"

    engine.save_test_results(test_results, test_file)
    engine.save_test_results(benchmark_results, benchmark_file)

    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_file}")
    print(f"âš¡ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: {benchmark_file}")


def run_interactive(args):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘")

    model_path = args.model_path or args.output_dir

    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ë¨¼ì € í›ˆë ¨ì„ ì™„ë£Œí•˜ê±°ë‚˜ ì˜¬ë°”ë¥¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    # ì¶”ë¡  ì—”ì§„ ë¡œë“œ
    engine = InferenceEngine()
    engine.load_finetuned_model(model_path, args.model_name)

    # ëŒ€í™”í˜• ì±„íŒ… ì‹œì‘
    chat_history = engine.interactive_chat()

    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    if chat_history:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chat_file = f"chat_history_{timestamp}.json"
        engine.save_test_results(chat_history, chat_file)
        print(f"\nì±„íŒ… ê¸°ë¡ ì €ì¥: {chat_file}")


def print_usage_examples():
    """ì‚¬ìš© ì˜ˆì‹œ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“š ê°œì„ ëœ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ")
    print("=" * 60)
    print()
    print("1. ê³ í’ˆì§ˆ í›ˆë ¨ (ê¶Œì¥):")
    print("   python improved_main.py --mode train --preset quality --epochs 8")
    print()
    print("2. ë¹ ë¥¸ í›ˆë ¨:")
    print("   python improved_main.py --mode train --preset fast --epochs 4")
    print()
    print("3. ë©”ëª¨ë¦¬ ì ˆì•½ í›ˆë ¨:")
    print("   python improved_main.py --mode train --preset memory_efficient")
    print()
    print("4. ì¶”ë¡  ëª¨ë“œ:")
    print("   python improved_main.py --mode interactive")
    print()
    print("5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:")
    print("   python improved_main.py --mode test")
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\n" + "=" * 60)
        print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("=" * 60)